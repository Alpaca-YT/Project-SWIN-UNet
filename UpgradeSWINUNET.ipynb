{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6837be7b-f8d4-45df-a478-bace933990d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alpaca_YT\\.cache\\huggingface\\hub\\models--timm--swin_base_patch4_window7_224.ms_in22k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始在 cuda 上训练最终修正版的 SwinUnet (使用高频加权损失)...\n",
      "Epoch 01/15 | Train Loss: 0.035127 | Val Loss: 0.016123\n",
      "Epoch 02/15 | Train Loss: 0.016485 | Val Loss: 0.015261\n",
      "Epoch 03/15 | Train Loss: 0.015985 | Val Loss: 0.015000\n",
      "Epoch 04/15 | Train Loss: 0.015907 | Val Loss: 0.014712\n",
      "Epoch 05/15 | Train Loss: 0.015558 | Val Loss: 0.014847\n",
      "Epoch 06/15 | Train Loss: 0.015098 | Val Loss: 0.014686\n",
      "Epoch 07/15 | Train Loss: 0.014871 | Val Loss: 0.014656\n",
      "Epoch 08/15 | Train Loss: 0.014888 | Val Loss: 0.014410\n",
      "Epoch 09/15 | Train Loss: 0.014616 | Val Loss: 0.014365\n",
      "Epoch 10/15 | Train Loss: 0.014673 | Val Loss: 0.014398\n",
      "Epoch 11/15 | Train Loss: 0.014506 | Val Loss: 0.014172\n",
      "Epoch 12/15 | Train Loss: 0.014388 | Val Loss: 0.014119\n",
      "Epoch 13/15 | Train Loss: 0.014222 | Val Loss: 0.014093\n",
      "Epoch 14/15 | Train Loss: 0.014208 | Val Loss: 0.014308\n",
      "Epoch 15/15 | Train Loss: 0.014201 | Val Loss: 0.013878\n",
      "\n",
      "训练完毕，所有模型已保存在 'Train_SwinUnet_UP_newlung/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 设置和数据加载部分 (保持不变)\n",
    "# ----------------------------------------\n",
    "# (此处省略与上次相同的 Dataset 定义和数据加载代码，以保持简洁)\n",
    "patches_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\lung_slices_dataset\\lung_slice_xy\"\n",
    "output_model_dir = \"Train_SwinUnet_UP_newlung\" # 使用新的文件夹以避免混淆\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "num_epochs = 15\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".jpg\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "class RotLowHighDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, all_fns_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns_list[i] for i in indices]\n",
    "    def __len__(self): return len(self.fns) * 2\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, rot_flag = idx // 2, idx % 2\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        arr = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        if rot_flag == 1: arr = np.rot90(arr, k=1)\n",
    "        down_arr = cv2.resize(arr, (256, 32), interpolation=cv2.INTER_AREA)\n",
    "        up_img = cv2.resize(down_arr, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        inp_t = self.transform(Image.fromarray(up_img))\n",
    "        tgt_t = self.transform(Image.fromarray(arr))\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "class PlainLowHighDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, all_fns_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns_list[i] for i in indices]\n",
    "    def __len__(self): return len(self.fns)\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        arr = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        down_arr = cv2.resize(arr, (256, 32), interpolation=cv2.INTER_AREA)\n",
    "        up_img = cv2.resize(down_arr, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        inp_t = self.transform(Image.fromarray(up_img))\n",
    "        tgt_t = self.transform(Image.fromarray(arr))\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义修正后的 SwinUnet 模型\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, 1, 1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class UpBlockPixelShuffle(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch * 4, 3, 1, 1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x): return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class SwinUnet(nn.Module):\n",
    "    def __init__(self, in_ch: int = 1, out_ch: int = 1, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "\n",
    "        # --- 主要变更点 1: 更换为Swin-Base模型 ---\n",
    "        self.encoder = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',  # <-- 从 'swin_tiny' 升级到 'swin_base'\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            in_chans=3,  # Swin Transformer 预训练模型通常需要3通道输入\n",
    "            img_size=256\n",
    "        )\n",
    "\n",
    "        # 获取Swin-Base模型在每个阶段输出的通道数\n",
    "        # Swin-Base的通道数通常为: [128, 256, 512, 1024]\n",
    "        encoder_channels: List[int] = self.encoder.feature_info.channels()\n",
    "\n",
    "        # --- 主要变更点 2: 调整解码器以匹配Swin-Base的通道数 ---\n",
    "        # 调整UpBlock和DoubleConv的输入/输出通道，以匹配encoder_channels\n",
    "        self.up4 = UpBlockPixelShuffle(encoder_channels[3], 512) # 1024 -> 512\n",
    "        self.dec4 = DoubleConv(encoder_channels[2] + 512, 512)   # 512 + 512 -> 512\n",
    "\n",
    "        self.up3 = UpBlockPixelShuffle(512, 256)\n",
    "        self.dec3 = DoubleConv(encoder_channels[1] + 256, 256)   # 256 + 256 -> 256\n",
    "\n",
    "        self.up2 = UpBlockPixelShuffle(256, 128)\n",
    "        self.dec2 = DoubleConv(encoder_channels[0] + 128, 128)   # 128 + 128 -> 128\n",
    "\n",
    "        self.up1 = UpBlockPixelShuffle(128, 64)\n",
    "        # 注意: 这里的DoubleConv输入通道数也需要调整\n",
    "        self.dec1 = DoubleConv(in_ch + 64, 64) # 假设最终与单通道原始输入拼接\n",
    "\n",
    "        self.outc = nn.Conv2d(64, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig = x\n",
    "        # 如果输入是单通道，则复制为3通道以适应预训练的编码器\n",
    "        if self.in_ch == 1 and x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        # timm的features_only=True输出的特征图通常是 (B, H, W, C) 格式\n",
    "        # 我们需要将其转换为 (B, C, H, W)\n",
    "        e1 = features[0].permute(0, 3, 1, 2)\n",
    "        e2 = features[1].permute(0, 3, 1, 2)\n",
    "        e3 = features[2].permute(0, 3, 1, 2)\n",
    "        e4 = features[3].permute(0, 3, 1, 2)\n",
    "\n",
    "        # 解码器路径\n",
    "        u4 = self.up4(e4)\n",
    "        d4 = self.dec4(torch.cat([u4, e3], dim=1))\n",
    "\n",
    "        u3 = self.up3(d4)\n",
    "        d3 = self.dec3(torch.cat([u3, e2], dim=1))\n",
    "\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e1], dim=1))\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        \n",
    "        # 最后的上采样和与原始输入的拼接\n",
    "        final_up = F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d1 = self.dec1(torch.cat([final_up, x_orig], dim=1))\n",
    "        \n",
    "        return self.outc(d1)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 准备训练 (保持不变)\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = RotLowHighDataset(patches_folder, train_idxs, all_fns, transform)\n",
    "val_dataset = PlainLowHighDataset(patches_folder, val_idxs, all_fns, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SwinUnet(in_ch=1, out_ch=1, pretrained=True).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 训练循环 (保持不变)\n",
    "# ----------------------------------------\n",
    "# ----------------------------------------\n",
    "# 准备 Dataloader, 模型, 优化器 (保持不变)\n",
    "# ----------------------------------------\n",
    "# (此处假设之前的代码已定义好 model, optimizer, train_loader, val_loader, device 等变量)\n",
    "# criterion 仍然是基础的MSE损失\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ### 修改点 1：定义高频损失所需组件 ###\n",
    "# 定义拉普拉斯卷积核，并将其发送到正确的设备\n",
    "laplacian_kernel = torch.tensor(\n",
    "    [[0.0, -1.0, 0.0],\n",
    "     [-1.0, 4.0, -1.0],\n",
    "     [0.0, -1.0, 0.0]],\n",
    "    device=device, dtype=torch.float32\n",
    ").view(1, 1, 3, 3)\n",
    "\n",
    "# 定义高频损失的权重\n",
    "lambda_hf = 0.5\n",
    "\n",
    "# 定义高频损失函数\n",
    "def high_freq_loss(pred, target):\n",
    "    \"\"\"计算预测和目标之间高频分量的MSE损失\"\"\"\n",
    "    pred_lap = F.conv2d(pred, laplacian_kernel, padding=1)\n",
    "    tgt_lap  = F.conv2d(target, laplacian_kernel, padding=1)\n",
    "    return F.mse_loss(pred_lap, tgt_lap)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 训练循环 (修改损失计算部分)\n",
    "# ----------------------------------------\n",
    "print(f\"开始在 {device} 上训练最终修正版的 SwinUnet (使用高频加权损失)...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)\n",
    "        \n",
    "        # ### 修改点 2：计算复合损失 ###\n",
    "        mse_train = criterion(out, tgt)\n",
    "        hf_train = high_freq_loss(out, tgt)\n",
    "        loss = mse_train + lambda_hf * hf_train # 总损失 = MSE + λ * 高频损失\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inp.size(0)\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v, tgt_v = inp_v.to(device), tgt_v.to(device)\n",
    "            out_v = model(inp_v)\n",
    "            \n",
    "            # ### 修改点 3：在验证时也使用相同的复合损失 ###\n",
    "            mse_val = criterion(out_v, tgt_v)\n",
    "            hf_val = high_freq_loss(out_v, tgt_v)\n",
    "            loss_v = mse_val + lambda_hf * hf_val\n",
    "\n",
    "            val_loss += loss_v.item() * inp_v.size(0)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # 保存模型权重时，可以加上标识，例如 \"hf\" 代表 high-frequency\n",
    "    ckpt_path = os.path.join(output_model_dir, f\"SwinUnet_v2_hf_epoch{epoch:02d}.pth\")\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "print(f\"\\n训练完毕，所有模型已保存在 '{output_model_dir}/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cedb4f0a-f1d5-4b06-81a8-c6bfa8470c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_37200\\870280465.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载模型权重: Train_SwinUnet_UP_newlung/SwinUnet_v2_hf_epoch15.pth\n",
      "\n",
      "在 cuda 上评估模型...\n",
      "\n",
      "-> 验证集结果:\n",
      "   - 平均 PSNR: 24.6470 dB\n",
      "   - 平均 SSIM: 0.6209\n",
      "\n",
      "-> 测试集结果:\n",
      "   - 平均 PSNR: 24.1537 dB\n",
      "   - 平均 SSIM: 0.5899\n",
      "\n",
      "已保存 5 组测试图像到 'lung_SwinNet_reconstructions/' 文件夹中供查看。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_metric\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义与训练时完全一致的模型和 Dataset 类\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class UpBlockPixelShuffle(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch * 4, 3, 1, 1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x): return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class SwinUnet(nn.Module):\n",
    "    def __init__(self, in_ch: int = 1, out_ch: int = 1, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "\n",
    "        # --- 主要变更点 1: 更换为Swin-Base模型 ---\n",
    "        self.encoder = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',  # <-- 从 'swin_tiny' 升级到 'swin_base'\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            in_chans=3,  # Swin Transformer 预训练模型通常需要3通道输入\n",
    "            img_size=256\n",
    "        )\n",
    "\n",
    "        # 获取Swin-Base模型在每个阶段输出的通道数\n",
    "        # Swin-Base的通道数通常为: [128, 256, 512, 1024]\n",
    "        encoder_channels: List[int] = self.encoder.feature_info.channels()\n",
    "\n",
    "        # --- 主要变更点 2: 调整解码器以匹配Swin-Base的通道数 ---\n",
    "        # 调整UpBlock和DoubleConv的输入/输出通道，以匹配encoder_channels\n",
    "        self.up4 = UpBlockPixelShuffle(encoder_channels[3], 512) # 1024 -> 512\n",
    "        self.dec4 = DoubleConv(encoder_channels[2] + 512, 512)   # 512 + 512 -> 512\n",
    "\n",
    "        self.up3 = UpBlockPixelShuffle(512, 256)\n",
    "        self.dec3 = DoubleConv(encoder_channels[1] + 256, 256)   # 256 + 256 -> 256\n",
    "\n",
    "        self.up2 = UpBlockPixelShuffle(256, 128)\n",
    "        self.dec2 = DoubleConv(encoder_channels[0] + 128, 128)   # 128 + 128 -> 128\n",
    "\n",
    "        self.up1 = UpBlockPixelShuffle(128, 64)\n",
    "        # 注意: 这里的DoubleConv输入通道数也需要调整\n",
    "        self.dec1 = DoubleConv(in_ch + 64, 64) # 假设最终与单通道原始输入拼接\n",
    "\n",
    "        self.outc = nn.Conv2d(64, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig = x\n",
    "        # 如果输入是单通道，则复制为3通道以适应预训练的编码器\n",
    "        if self.in_ch == 1 and x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        # timm的features_only=True输出的特征图通常是 (B, H, W, C) 格式\n",
    "        # 我们需要将其转换为 (B, C, H, W)\n",
    "        e1 = features[0].permute(0, 3, 1, 2)\n",
    "        e2 = features[1].permute(0, 3, 1, 2)\n",
    "        e3 = features[2].permute(0, 3, 1, 2)\n",
    "        e4 = features[3].permute(0, 3, 1, 2)\n",
    "\n",
    "        # 解码器路径\n",
    "        u4 = self.up4(e4)\n",
    "        d4 = self.dec4(torch.cat([u4, e3], dim=1))\n",
    "\n",
    "        u3 = self.up3(d4)\n",
    "        d3 = self.dec3(torch.cat([u3, e2], dim=1))\n",
    "\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e1], dim=1))\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        \n",
    "        # 最后的上采样和与原始输入的拼接\n",
    "        final_up = F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d1 = self.dec1(torch.cat([final_up, x_orig], dim=1))\n",
    "        \n",
    "        return self.outc(d1)\n",
    "\n",
    "\n",
    "# ### 修改点 1：让 Dataset 返回文件名 ###\n",
    "class PlainLowHighDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, all_fns_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder, self.transform = patches_folder, transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns_list[i] for i in indices]\n",
    "    def __len__(self): return len(self.fns)\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        arr = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        down_arr = cv2.resize(arr, (256, 32), interpolation=cv2.INTER_AREA)\n",
    "        up_img = cv2.resize(down_arr, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        inp_t = self.transform(Image.fromarray(up_img))\n",
    "        tgt_t = self.transform(Image.fromarray(arr))\n",
    "        return inp_t, tgt_t, fn # 返回文件名\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 设置路径和参数\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.ToTensor()\n",
    "# ### 新增：定义保存图像的文件夹和数量 ###\n",
    "RECONSTRUCTION_DIR = \"lung_SwinNet_reconstructions\"\n",
    "IMAGES_TO_SAVE = 5\n",
    "os.makedirs(RECONSTRUCTION_DIR, exist_ok=True)\n",
    "\n",
    "# 准备 Dataloaders (与之前相同)\n",
    "val_patches_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\lung_slices_dataset\\lung_slice_xy\"\n",
    "all_val_fns = sorted([f for f in os.listdir(val_patches_folder) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "all_val_indices = list(range(len(all_val_fns)))\n",
    "_, val_idxs = train_test_split(all_val_indices, test_size=0.25, random_state=42)\n",
    "val_dataset = PlainLowHighDataset(val_patches_folder, val_idxs, all_val_fns, transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_patches_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\lung_slices_dataset\\lung_slice_yz\"\n",
    "all_test_fns = sorted([f for f in os.listdir(test_patches_folder) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "all_test_indices = list(range(len(all_test_fns)))\n",
    "test_dataset = PlainLowHighDataset(test_patches_folder, all_test_indices, all_test_fns, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 加载模型权重 (与之前相同)\n",
    "# ----------------------------------------\n",
    "model = SwinUnet(in_ch=1, out_ch=1, pretrained=False).to(device)\n",
    "checkpoint_path = r\"Train_SwinUnet_UP_newlung/SwinUnet_v2_hf_epoch15.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"成功加载模型权重: {checkpoint_path}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义评估函数 (增加保存逻辑)\n",
    "# ----------------------------------------\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"将 [0,1] 范围的Tensor转换为可保存的PIL Image对象\"\"\"\n",
    "    arr = tensor.clamp(0, 1).cpu().squeeze().numpy()\n",
    "    return Image.fromarray((arr * 255).round().astype(np.uint8))\n",
    "\n",
    "# ### 修改点 2：为函数增加保存功能相关的参数 ###\n",
    "def evaluate_model(loader, model_to_test, output_dir=None, save_count=0):\n",
    "    total_psnr, total_ssim, count = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        # ### 修改点 3：使用 enumerate 以获取索引 ###\n",
    "        for i, (inp_t, tgt_t, fn) in enumerate(loader):\n",
    "            # 解包文件名元组（如果dataloader返回的是元组）\n",
    "            fn = fn[0] if isinstance(fn, (list, tuple)) else fn\n",
    "\n",
    "            inp_t, tgt_t = inp_t.to(device), tgt_t.to(device)\n",
    "            out_t = model_to_test(inp_t)\n",
    "            \n",
    "            pred_np = out_t.clamp(0, 1).cpu().squeeze().numpy()\n",
    "            target_np = tgt_t.cpu().squeeze().numpy()\n",
    "            \n",
    "            total_psnr += psnr_metric(target_np, pred_np, data_range=1.0)\n",
    "            total_ssim += ssim_metric(target_np, pred_np, data_range=1.0, channel_axis=None)\n",
    "            count += 1\n",
    "            \n",
    "            # ### 修改点 4：保存指定数量的图像 ###\n",
    "            if output_dir is not None and i < save_count:\n",
    "                base_name = os.path.splitext(fn)[0]\n",
    "                # 保存三种图像以供对比\n",
    "                tensor_to_image(inp_t).save(os.path.join(output_dir, f\"{base_name}_01_input.png\"))\n",
    "                tensor_to_image(out_t).save(os.path.join(output_dir, f\"{base_name}_02_reconstructed.png\"))\n",
    "                tensor_to_image(tgt_t).save(os.path.join(output_dir, f\"{base_name}_03_ground_truth.png\"))\n",
    "\n",
    "    return total_psnr / count, total_ssim / count\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 执行评估并打印结果\n",
    "# ----------------------------------------\n",
    "print(f\"\\n在 {device} 上评估模型...\")\n",
    "\n",
    "# 评估验证集，不保存图像\n",
    "val_psnr, val_ssim = evaluate_model(val_loader, model)\n",
    "print(f\"\\n-> 验证集结果:\")\n",
    "print(f\"   - 平均 PSNR: {val_psnr:.4f} dB\")\n",
    "print(f\"   - 平均 SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "# ### 修改点 5：评估测试集，并传入保存图像的参数 ###\n",
    "test_psnr, test_ssim = evaluate_model(test_loader, model, output_dir=RECONSTRUCTION_DIR, save_count=IMAGES_TO_SAVE)\n",
    "print(f\"\\n-> 测试集结果:\")\n",
    "print(f\"   - 平均 PSNR: {test_psnr:.4f} dB\")\n",
    "print(f\"   - 平均 SSIM: {test_ssim:.4f}\")\n",
    "print(f\"\\n已保存 {IMAGES_TO_SAVE} 组测试图像到 '{RECONSTRUCTION_DIR}/' 文件夹中供查看。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3735b194-3f36-40cf-9163-ddfad9b7fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件图像 PSNR: 23.85 dB\n",
      "文件图像 SSIM: 0.5130\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics(image_true, image_test):\n",
    "    \"\"\"\n",
    "    计算两张图像之间的 PSNR 和 SSIM.\n",
    "\n",
    "    参数:\n",
    "    image_true (np.ndarray): 真实图像 (参考图像).\n",
    "    image_test (np.ndarray): 测试图像 (待评估图像).\n",
    "\n",
    "    返回:\n",
    "    tuple: (psnr_value, ssim_value).\n",
    "    \"\"\"\n",
    "    # 1. 检查图像尺寸是否一致\n",
    "    if image_true.shape != image_test.shape:\n",
    "        raise ValueError(\"输入图像的尺寸必须完全相同。\")\n",
    "\n",
    "    # 2. 计算 PSNR\n",
    "    # data_range 是图像像素值的可能范围。对于8位图像，通常是 255。\n",
    "    psnr_value = psnr(image_true, image_test, data_range=255)\n",
    "\n",
    "    # 3. 计算 SSIM\n",
    "    # SSIM 通常在灰度图上计算。如果图像是彩色的，我们先将其转换为灰度图。\n",
    "    # scikit-image 的 ssim 函数也可以处理多通道图像，只需设置 channel_axis=-1\n",
    "    # 但最常见的做法还是先转为灰度图。\n",
    "    if image_true.ndim == 3:\n",
    "        gray_true = cv2.cvtColor(image_true, cv2.COLOR_BGR2GRAY)\n",
    "        gray_test = cv2.cvtColor(image_test, cv2.COLOR_BGR2GRAY)\n",
    "        ssim_value = ssim(gray_true, gray_test, data_range=255)\n",
    "    else:\n",
    "        # 如果已经是灰度图\n",
    "        ssim_value = ssim(image_true, image_test, data_range=255)\n",
    "        \n",
    "    return psnr_value, ssim_value\n",
    "\n",
    "\n",
    "\n",
    "path_true = r\"test1/Cropped_img01576.png\"\n",
    "path_test = r\"test1/I3-1576.png\"\n",
    "\n",
    "try:\n",
    "    # 使用 OpenCV 读取图片\n",
    "    img_true = cv2.imread(path_true)\n",
    "    img_test = cv2.imread(path_test)\n",
    "\n",
    "    # 检查图片是否成功加载\n",
    "    if img_true is None or img_test is None:\n",
    "        raise IOError(\"无法加载一张或多张图片，请检查路径是否正确。\")\n",
    "    \n",
    "    # 如果图片尺寸不同，可以先将其调整为一致（例如，调整为第一张图片的尺寸）\n",
    "    if img_true.shape != img_test.shape:\n",
    "        print(f\"警告：图片尺寸不同，将第二张图片调整为与第一张相同 ({img_true.shape})。\")\n",
    "        img_test = cv2.resize(img_test, (img_true.shape[1], img_true.shape[0]))\n",
    "\n",
    "    psnr_file, ssim_file = calculate_metrics(img_true, img_test)\n",
    "    print(f\"文件图像 PSNR: {psnr_file:.2f} dB\")\n",
    "    print(f\"文件图像 SSIM: {ssim_file:.4f}\")\n",
    "\n",
    "except (IOError, ValueError) as e:\n",
    "    print(f\"处理文件时发生错误: {e}\")\n",
    "    print(\"请将 'path/to/your/...' 替换为您的真实图片路径后重试。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eec6f-498f-4c91-8448-da75c0075d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
