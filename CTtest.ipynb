{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8875e793-a9a9-49ea-acee-42bd794edad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch sampling complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# 采样函数：从指定文件范围内的图像中，在中心区域随机截取固定大小的patch\n",
    "def sample_center_patches(\n",
    "    img_folder, \n",
    "    filename_template, \n",
    "    start_idx, \n",
    "    end_idx, \n",
    "    num_patches, \n",
    "    patch_size, \n",
    "    out_folder\n",
    "):\n",
    "    \"\"\"\n",
    "    img_folder: 图片所在文件夹\n",
    "    filename_template: 模板，如 \"YZ_{:04d}.png\" 或 \"image_{:05d}.png\"\n",
    "    start_idx, end_idx: 文件名的开始和结束索引（inclusive）\n",
    "    num_patches: 总共要采样的patch数\n",
    "    patch_size: patch的宽高（正方形）\n",
    "    out_folder: 保存patch的文件夹\n",
    "    \"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    indices = list(range(start_idx, end_idx + 1))\n",
    "    num_images = len(indices)\n",
    "    patches_per_image = num_patches // num_images\n",
    "    extra = num_patches % num_images\n",
    "\n",
    "    count = 0\n",
    "    for idx in indices:\n",
    "        fn = filename_template.format(idx)\n",
    "        path = os.path.join(img_folder, fn)\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        W, H = img.size\n",
    "        # 中心区域范围（确保patch完全在图像内部）\n",
    "        x_min = (W - patch_size) // 2 - patch_size // 2\n",
    "        y_min = (H - patch_size) // 2 - patch_size // 2\n",
    "        x_min = max(0, x_min)\n",
    "        y_min = max(0, y_min)\n",
    "        x_max = W - patch_size\n",
    "        y_max = H - patch_size\n",
    "\n",
    "        # 本图像需要采样的patch数\n",
    "        k = patches_per_image + (1 if extra > 0 else 0)\n",
    "        if extra > 0: extra -= 1\n",
    "\n",
    "        for i in range(k):\n",
    "            # 随机取样中心区域内的位置\n",
    "            x = random.randint(x_min, x_max)\n",
    "            y = random.randint(y_min, y_max)\n",
    "            patch = img.crop((x, y, x + patch_size, y + patch_size))\n",
    "            out_name = f\"{os.path.splitext(fn)[0]}_patch{count:04d}.png\"\n",
    "            patch.save(os.path.join(out_folder, out_name))\n",
    "            count += 1\n",
    "            if count >= num_patches:\n",
    "                return\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 测试集：YZ_1000 → YZ_1256，共1024 patch\n",
    "# ----------------------------\n",
    "sample_center_patches(\n",
    "    img_folder=r\"C:\\Users\\Alpaca_YT\\pythonSet\\post_reconstruct_YZ\",\n",
    "    filename_template=\"YZ_{:04d}.png\",\n",
    "    start_idx=1000,\n",
    "    end_idx=1256,\n",
    "    num_patches=1024,\n",
    "    patch_size=256,\n",
    "    out_folder=r\"C:\\Users\\Alpaca_YT\\pythonSet\\test_patches_YZ\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 训练集：data2 image_01601 → image_01856，共1024 patch\n",
    "# ----------------------------\n",
    "sample_center_patches(\n",
    "    img_folder=r\"C:\\Users\\Alpaca_YT\\pythonSet\\data2\",\n",
    "    filename_template=\"image_{:05d}.png\",\n",
    "    start_idx=1601,\n",
    "    end_idx=1856,\n",
    "    num_patches=1024,\n",
    "    patch_size=256,\n",
    "    out_folder=r\"C:\\Users\\Alpaca_YT\\pythonSet\\train_patches_XY\"\n",
    ")\n",
    "\n",
    "print(\"Patch sampling complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caede11d-c407-4cc2-8325-3b5cb1cfe31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear interp:   Avg PSNR = 15.60 dB, Avg SSIM = 0.3897\n",
      "Bilinear interp: Avg PSNR = 14.60 dB, Avg SSIM = 0.2912\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "test_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\test_patches_YZ\"\n",
    "linear_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\output_linear\"\n",
    "bilinear_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\output_bilinear\"\n",
    "\n",
    "os.makedirs(linear_folder, exist_ok=True)\n",
    "os.makedirs(bilinear_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Select first 100 test images\n",
    "# ----------------------------\n",
    "filenames = sorted([f for f in os.listdir(test_folder) if f.lower().endswith('.png')])[:400]\n",
    "\n",
    "psnr_lin_list, ssim_lin_list = [], []\n",
    "psnr_bi_list, ssim_bi_list = [], []\n",
    "\n",
    "# ----------------------------\n",
    "# Process each image\n",
    "# ----------------------------\n",
    "for fn in filenames:\n",
    "    # Load original 256x256 patch\n",
    "    orig = np.array(Image.open(os.path.join(test_folder, fn)).convert(\"L\"))\n",
    "    H, W = orig.shape\n",
    "\n",
    "    # 8x downsample vertically\n",
    "    down = orig[::8, :]\n",
    "\n",
    "    # 1D linear interpolation along vertical axis\n",
    "    old_x = np.arange(down.shape[0]) * 8\n",
    "    new_x = np.arange(H)\n",
    "    lin = np.zeros_like(orig, dtype=np.float32)\n",
    "    for c in range(W):\n",
    "        lin[:, c] = np.interp(new_x, old_x, down[:, c])\n",
    "    lin = np.clip(lin, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # 2D bilinear interpolation using PIL\n",
    "    img_down = Image.fromarray(down)\n",
    "    bi = img_down.resize((W, H), resample=Image.BILINEAR)\n",
    "    bi = np.array(bi)\n",
    "\n",
    "    # Save outputs\n",
    "    Image.fromarray(lin).save(os.path.join(linear_folder, fn))\n",
    "    Image.fromarray(bi).save(os.path.join(bilinear_folder, fn))\n",
    "\n",
    "    # Compute metrics\n",
    "    psnr_lin = peak_signal_noise_ratio(orig, lin, data_range=255)\n",
    "    ssim_lin = structural_similarity(orig, lin, data_range=255)\n",
    "    psnr_bi  = peak_signal_noise_ratio(orig, bi,  data_range=255)\n",
    "    ssim_bi  = structural_similarity(orig, bi,  data_range=255)\n",
    "\n",
    "    psnr_lin_list.append(psnr_lin); ssim_lin_list.append(ssim_lin)\n",
    "    psnr_bi_list.append(psnr_bi);   ssim_bi_list.append(ssim_bi)\n",
    "\n",
    "# ----------------------------\n",
    "# Print average metrics\n",
    "# ----------------------------\n",
    "print(f\"Linear interp:   Avg PSNR = {np.mean(psnr_lin_list):.2f} dB, Avg SSIM = {np.mean(ssim_lin_list):.4f}\")\n",
    "print(f\"Bilinear interp: Avg PSNR = {np.mean(psnr_bi_list):.2f} dB, Avg SSIM = {np.mean(ssim_bi_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c4c559-9e0f-4033-9de2-49c04c950cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16384 samples in ./data/up8\\train\\in and ./data/up8\\train\\gt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    107\u001b[0m out \u001b[38;5;241m=\u001b[39m model(inp)\n\u001b[1;32m--> 108\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(out, tgt)\n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    110\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\torch\\nn\\functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 生成八倍下采样数据对 (train only)\n",
    "# ----------------------------\n",
    "orig_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\train_patches_XY\"\n",
    "out_base = \"./data/up8\"\n",
    "in_dir = os.path.join(out_base, \"train\", \"in\")\n",
    "gt_dir = os.path.join(out_base, \"train\", \"gt\")\n",
    "os.makedirs(in_dir, exist_ok=True)\n",
    "os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "for fn in sorted(os.listdir(orig_folder)):\n",
    "    if not fn.lower().endswith(\".png\"): continue\n",
    "    arr = np.array(Image.open(os.path.join(orig_folder, fn)).convert(\"L\"))\n",
    "    for rot in (0, 1):\n",
    "        arr_r = np.rot90(arr, k=1) if rot else arr\n",
    "        for p in range(8):\n",
    "            inp = arr_r[p::8, :]\n",
    "            gt  = arr_r\n",
    "            name = f\"{fn[:-4]}_r{rot}_p{p}.png\"\n",
    "            Image.fromarray(inp).save(os.path.join(in_dir, name))\n",
    "            Image.fromarray(gt).save(os.path.join(gt_dir,  name))\n",
    "print(f\"Generated {len(os.listdir(in_dir))} samples in {in_dir} and {gt_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4585c0-f749-4fbc-9692-a30b91a36afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16384 samples in:\n",
      "  in: ./data/up8\\train\\in\n",
      "  gt: ./data/up8\\train\\gt\n",
      "Total samples: 16384, Train: 12288, Val: 4096\n",
      "Epoch 01 | Train Loss: 0.0026 | Val Loss: 0.0018\n",
      "Epoch 02 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 03 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 04 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 05 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 06 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 07 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 08 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 09 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 10 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 11 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 12 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 13 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 14 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Epoch 15 | Train Loss: 0.0017 | Val Loss: 0.0017\n",
      "Training complete. Models saved in Train_UP8_phases/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 生成八倍下采样数据对（如果尚未生成）\n",
    "# ----------------------------\n",
    "orig_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\train_patches_XY\"\n",
    "out_base    = \"./data/up8\"\n",
    "in_dir      = os.path.join(out_base, \"train\", \"in\")\n",
    "gt_dir      = os.path.join(out_base, \"train\", \"gt\")\n",
    "os.makedirs(in_dir, exist_ok=True)\n",
    "os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "# 仅在第一次运行时生成：每张 256×256 原图 → 8× 下采样 (32×256)\n",
    "for fn in sorted(os.listdir(orig_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "    arr = np.array(Image.open(os.path.join(orig_folder, fn)).convert(\"L\"))\n",
    "    for rot in (0, 1):\n",
    "        arr_r = np.rot90(arr, k=1) if rot else arr\n",
    "        for p in range(8):\n",
    "            # 下采样相位 p：从行索引 p, p+8, p+16, … 提取\n",
    "            inp = arr_r[p::8, :]             # shape [32,256]\n",
    "            gt  = arr_r                     # shape [256,256]\n",
    "            name = f\"{fn[:-4]}_r{rot}_p{p}.png\"\n",
    "            Image.fromarray(inp).save(os.path.join(in_dir, name))\n",
    "            Image.fromarray(gt).save(os.path.join(gt_dir,  name))\n",
    "\n",
    "print(f\"Generated {len(os.listdir(in_dir))} samples in:\\n  in: {in_dir}\\n  gt: {gt_dir}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Dataset & DataLoader (含 25% 验证集)\n",
    "# ----------------------------\n",
    "class PairDataset8(Dataset):\n",
    "    \"\"\"加载 (下采样32×256, 原始256×256) 数据对。\"\"\"\n",
    "    def __init__(self, in_folder, gt_folder, transform=None):\n",
    "        super().__init__()\n",
    "        self.in_folder = in_folder\n",
    "        self.gt_folder = gt_folder\n",
    "        self.fns = sorted([f for f in os.listdir(in_folder) if f.endswith('.png')])\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        inp = Image.open(os.path.join(self.in_folder, fn)).convert(\"L\")\n",
    "        gt  = Image.open(os.path.join(self.gt_folder, fn)).convert(\"L\")\n",
    "        return self.transform(inp), self.transform(gt)\n",
    "\n",
    "# 加载整个数据集\n",
    "full_dataset = PairDataset8(in_dir, gt_dir, transform=transforms.ToTensor())\n",
    "\n",
    "# 按 75/25 划分训练/验证\n",
    "n_total = len(full_dataset)\n",
    "n_val   = n_total // 4  # 25%\n",
    "n_train = n_total - n_val\n",
    "train_ds, val_ds = random_split(full_dataset, [n_train, n_val],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Total samples: {n_total}, Train: {n_train}, Val: {n_val}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 定义 UNetUp8（含 8× 上采样）\n",
    "# ----------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入：[B,1,32,256] → 输出：[B,1,256,256]\n",
    "    通过七次逐步纵向×2 解码 (4 次拼接跳跃 + 3 次纯上采样)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder 部分：32→16→8→4→2\n",
    "        self.enc1 = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2,1))          # [B,base,32,256] → [B,base,16,256]\n",
    "        self.enc2 = DoubleConv(base, base*2)\n",
    "        self.pool2 = nn.MaxPool2d((2,1))          # [B,base*2,16,256] → [B,base*2,8,256]\n",
    "        self.enc3 = DoubleConv(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool2d((2,1))          # [B,base*4,8,256] → [B,base*4,4,256]\n",
    "        self.enc4 = DoubleConv(base*4, base*8)\n",
    "        self.pool4 = nn.MaxPool2d((2,1))          # [B,base*8,4,256] → [B,base*8,2,256]\n",
    "\n",
    "        # Bottleneck：在 2×256\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        # Decoder 部分：2→4→8→16→32\n",
    "        self.up4 = nn.ConvTranspose2d(base*16, base*8, (2,1), (2,1))\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "        self.up3 = nn.ConvTranspose2d(base*8, base*4, (2,1), (2,1))\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, (2,1), (2,1))\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base,   (2,1), (2,1))\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        # 额外三次纵向上采样：32→64→128→256\n",
    "        self.up_ex1 = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2 = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3 = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # 最终 1×1 卷积\n",
    "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        e1 = self.enc1(x)            # → [B,base,32,256]\n",
    "        p1 = self.pool1(e1)          # → [B,base,16,256]\n",
    "        e2 = self.enc2(p1)           # → [B,base*2,16,256]\n",
    "        p2 = self.pool2(e2)          # → [B,base*2,8,256]\n",
    "        e3 = self.enc3(p2)           # → [B,base*4,8,256]\n",
    "        p3 = self.pool3(e3)          # → [B,base*4,4,256]\n",
    "        e4 = self.enc4(p3)           # → [B,base*8,4,256]\n",
    "        p4 = self.pool4(e4)          # → [B,base*8,2,256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B,base*16,2,256]\n",
    "\n",
    "        # 解码 (跳跃连接)\n",
    "        u4 = self.up4(b)                             # → [B,base*8,4,256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))   # → [B,base*8,4,256]\n",
    "        u3 = self.up3(d4)                            # → [B,base*4,8,256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))   # → [B,base*4,8,256]\n",
    "        u2 = self.up2(d3)                            # → [B,base*2,16,256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))   # → [B,base*2,16,256]\n",
    "        u1 = self.up1(d2)                            # → [B,base,32,256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))   # → [B,base,32,256]\n",
    "\n",
    "        # 额外三次纵向上采样 (不再拼接跳跃，仅逐层上采样+卷积)\n",
    "        u_ex1 = self.up_ex1(d1)           # → [B,base,64,256]\n",
    "        d_ex1 = self.dec_ex1(u_ex1)      # → [B,base,64,256]\n",
    "        u_ex2 = self.up_ex2(d_ex1)        # → [B,base,128,256]\n",
    "        d_ex2 = self.dec_ex2(u_ex2)      # → [B,base,128,256]\n",
    "        u_ex3 = self.up_ex3(d_ex2)        # → [B,base,256,256]\n",
    "        d_ex3 = self.dec_ex3(u_ex3)      # → [B,base,256,256]\n",
    "\n",
    "        out = self.outc(d_ex3)            # → [B,1,256,256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 训练 UNetUp8（包含 25% 验证集）\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "os.makedirs(\"Train_UP8_phases\", exist_ok=True)\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)                 # [B,1,256,256]\n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * inp.size(0)\n",
    "    avg_train_loss = total_train_loss / len(train_ds)\n",
    "\n",
    "    # 验证模式\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in val_loader:\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            out = model(inp)             # [B,1,256,256]\n",
    "            loss = criterion(out, tgt)\n",
    "            total_val_loss += loss.item() * inp.size(0)\n",
    "    avg_val_loss = total_val_loss / len(val_ds)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # 如果验证损失更好，则保存权重\n",
    "    ckpt_path = f\"Train_UP8_phases/UnetUp8_epoch{epoch:02d}.pth\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # 同步保存最佳模型\n",
    "        torch.save(model.state_dict(), \"Train_UP8_phases/UnetUp8_best.pth\")\n",
    "\n",
    "print(\"Training complete. Models saved in Train_UP8_phases/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835e7ae-604e-4909-8a7b-83039cf15b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a581ce7c-1330-4b6c-9423-c5d6138ce9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_19468\\586950527.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Split Average PSNR: 28.47 dB\n",
      "Validation Split Average SSIM: 0.6343\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torchvision import transforms\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 定义模型结构（与训练时一致）\n",
    "# ----------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet8x(nn.Module):\n",
    "    def __init__(self, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 256→128→64→32\n",
    "        self.enc1, self.pool1 = DoubleConv(1, base),   nn.MaxPool2d(2)\n",
    "        self.enc2, self.pool2 = DoubleConv(base, base*2), nn.MaxPool2d(2)\n",
    "        self.enc3, self.pool3 = DoubleConv(base*2, base*4), nn.MaxPool2d(2)\n",
    "        # Bottleneck at 32×256\n",
    "        self.bottleneck = DoubleConv(base*4, base*8)\n",
    "        # Decoder: 32→64→128→256\n",
    "        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base,   2, stride=2)\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "        self.outc = nn.Conv2d(base, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1); p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2); p3 = self.pool3(e3)\n",
    "        b  = self.bottleneck(p3)\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = self.outc(d1)  # [B,1,32,256]\n",
    "        # 8× 上采样回 [B,1,256,256]\n",
    "        return F.interpolate(out, scale_factor=(8,1), mode='bilinear', align_corners=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 加载模型权重\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet8x().to(device)\n",
    "ckpt_path = \"Train_UP8_phases/Unet8x_epoch2.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 准备验证集列表 (train_patches_XY 25%)\n",
    "# ----------------------------\n",
    "patch_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patch_folder) if f.lower().endswith('.png')])\n",
    "num_val = int(len(all_fns) * 0.25)\n",
    "val_fns = all_fns[:num_val]  # 前 25% 作为验证\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 创建输出文件夹\n",
    "# ----------------------------\n",
    "output_folder = \"./output_UP8_phases_val_split\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 评估指标累计\n",
    "# ----------------------------\n",
    "to_tensor = transforms.ToTensor()\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "# ----------------------------\n",
    "# 6. 验证集测试循环\n",
    "# ----------------------------\n",
    "for fn in val_fns:\n",
    "    # 读取原始 256×256 补丁\n",
    "    gt_img = Image.open(os.path.join(patch_folder, fn)).convert(\"L\")\n",
    "    gt_arr = np.array(gt_img)\n",
    "\n",
    "    # 8× 下采样\n",
    "    inp_arr = gt_arr[::8, :]\n",
    "\n",
    "    # 转 tensor [1,1,32,256]\n",
    "    inp_t = to_tensor(inp_arr)[None].to(device)\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)\n",
    "\n",
    "    # 转 numpy [256,256]\n",
    "    pred = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 保存重建图像\n",
    "    Image.fromarray(pred).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 计算 PSNR/SSIM\n",
    "    psnr_val = peak_signal_noise_ratio(gt_arr, pred, data_range=255)\n",
    "    ssim_val = structural_similarity(gt_arr, pred, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "  \n",
    "\n",
    "# ----------------------------\n",
    "# 7. 输出平均指标\n",
    "# ----------------------------\n",
    "print(f\"Validation Split Average PSNR: {total_psnr/count:.2f} dB\")\n",
    "print(f\"Validation Split Average SSIM: {total_ssim/count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bcce297-c9e1-4c85-8660-dec75af336aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_19468\\3634783539.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average PSNR: 15.39 dB\n",
      "Validation Average SSIM: 0.2788\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torchvision import transforms\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 定义模型结构（与训练时一致）\n",
    "# ----------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "class UNet8x(nn.Module):\n",
    "    def __init__(self, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 256→128→64→32\n",
    "        self.enc1, self.pool1 = DoubleConv(1, base),   nn.MaxPool2d(2)\n",
    "        self.enc2, self.pool2 = DoubleConv(base, base*2), nn.MaxPool2d(2)\n",
    "        self.enc3, self.pool3 = DoubleConv(base*2, base*4), nn.MaxPool2d(2)\n",
    "        # Bottleneck at 32×256\n",
    "        self.bottleneck = DoubleConv(base*4, base*8)\n",
    "        # Decoder: 32→64→128→256\n",
    "        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base,   2, stride=2)\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "        self.outc = nn.Conv2d(base, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1); p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2); p3 = self.pool3(e3)\n",
    "        b  = self.bottleneck(p3)\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = self.outc(d1)  # [B,1,32,256]\n",
    "        # 8× 上采样回 [B,1,256,256]\n",
    "        return F.interpolate(out, scale_factor=(8,1), mode='bilinear', align_corners=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 加载训练好的模型（第15 epoch）\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet8x().to(device)\n",
    "ckpt_path = \"Train_UP8_phases/Unet8x_epoch3.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 在 validation set 上测试\n",
    "#    (使用之前生成的 test_patches_YZ)\n",
    "# ----------------------------\n",
    "test_folder   = r\"C:\\Users\\Alpaca_YT\\pythonSet\\test_patches_YZ\"\n",
    "output_folder = \"./output_UP8_phases_val\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith('.png'):\n",
    "        continue\n",
    "    # 读取「原始」256×256 patch\n",
    "    gt_img = Image.open(os.path.join(test_folder, fn)).convert(\"L\")\n",
    "    gt_arr = np.array(gt_img)\n",
    "\n",
    "    # 8× 下采样\n",
    "    inp_arr = gt_arr[::8, :]\n",
    "\n",
    "    # 转 tensor 并放到 GPU\n",
    "    inp_t = to_tensor(inp_arr)[None].to(device)  # shape [1,1,32,256]\n",
    "\n",
    "    # 模型前向\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256]\n",
    "\n",
    "    # 转为 uint8\n",
    "    pred = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0,255).astype(np.uint8)\n",
    "\n",
    "    # 保存重建结果\n",
    "    Image.fromarray(pred).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(gt_arr, pred, data_range=255)\n",
    "    ssim_val = structural_similarity(gt_arr, pred, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均结果\n",
    "print(f\"Validation Average PSNR: {total_psnr/count:.2f} dB\")\n",
    "print(f\"Validation Average SSIM: {total_ssim/count:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680db241-c645-404b-ab81-b88bbe78bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 1536 (含旋转增强×2), 验证集: 256 (无增强)\n",
      "Epoch 01 | Train Loss: 0.0063 | Val Loss: 0.0024\n",
      "Epoch 02 | Train Loss: 0.0021 | Val Loss: 0.0019\n",
      "Epoch 03 | Train Loss: 0.0018 | Val Loss: 0.0017\n",
      "Epoch 04 | Train Loss: 0.0017 | Val Loss: 0.0016\n",
      "Epoch 05 | Train Loss: 0.0016 | Val Loss: 0.0016\n",
      "Epoch 06 | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch 07 | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch 08 | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch 09 | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch 10 | Train Loss: 0.0015 | Val Loss: 0.0015\n",
      "Epoch 11 | Train Loss: 0.0014 | Val Loss: 0.0015\n",
      "Epoch 12 | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch 13 | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch 14 | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "Epoch 15 | Train Loss: 0.0014 | Val Loss: 0.0014\n",
      "训练完毕，模型保存在 Train_UP8_rotonly/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转增强（×2）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotOnly8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    仅对原图做 0° 和 90° 旋转两种版本，输入是 1/8 下采样图 (32×256)，\n",
    "    目标是完整旋转后原图 (256×256)。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        # 构建文件名列表\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每张原图返回两个样本：rot=0 或 rot=1\n",
    "        return len(self.fns) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 2\n",
    "        rot_flag = idx % 2  # 0→不旋转, 1→逆时针90°\n",
    "\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        # 旋转（如果需要）\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        # 下采样：竖向每 8 行保留一行 → 32×256\n",
    "        down_arr = arr[::8, :]\n",
    "\n",
    "        # 转为 PIL Image\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        # ToTensor: 自动归一化到 [0,1], shape: [C=1,H,W]\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义仅旋转增强的验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    对验证集，不做任何旋转，仅 1/8 下采样与原图配对。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义 UNetUp8 模型（与训练时一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入 [B,1,32,256] → 输出 [B,1,256,256]\n",
    "    七次纵向 ×2 的可训练上采样（含跳跃连接）。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2,1))\n",
    "        self.enc2  = DoubleConv(base, base*2)\n",
    "        self.pool2 = nn.MaxPool2d((2,1))\n",
    "        self.enc3  = DoubleConv(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool2d((2,1))\n",
    "        self.enc4  = DoubleConv(base*4, base*8)\n",
    "        self.pool4 = nn.MaxPool2d((2,1))\n",
    "\n",
    "        # Bottleneck at 2×256\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        # Decoder 2→4→8→16→32 (跳跃连接)\n",
    "        self.up4  = nn.ConvTranspose2d(base*16, base*8, (2,1), (2,1))\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "        self.up3  = nn.ConvTranspose2d(base*8, base*4, (2,1), (2,1))\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2  = nn.ConvTranspose2d(base*4, base*2, (2,1), (2,1))\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1  = nn.ConvTranspose2d(base*2, base,   (2,1), (2,1))\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        # 额外三次纵向可训练上采样：32→64→128→256\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # 最后 1×1 卷积\n",
    "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        e1 = self.enc1(x)     \n",
    "        p1 = self.pool1(e1)   \n",
    "        e2 = self.enc2(p1)    \n",
    "        p2 = self.pool2(e2)   \n",
    "        e3 = self.enc3(p2)    \n",
    "        p3 = self.pool3(e3)   \n",
    "        e4 = self.enc4(p3)    \n",
    "        p4 = self.pool4(e4)   \n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)  \n",
    "\n",
    "        # 解码 (跳跃连接)\n",
    "        u4 = self.up4(b)                            \n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))  \n",
    "        u3 = self.up3(d4)                           \n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))  \n",
    "        u2 = self.up2(d3)                           \n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))  \n",
    "        u1 = self.up1(d2)                           \n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))  \n",
    "\n",
    "        # 额外三次可训练上采样\n",
    "        u_ex1 = self.up_ex1(d1)       \n",
    "        d_ex1 = self.dec_ex1(u_ex1)  \n",
    "        u_ex2 = self.up_ex2(d_ex1)    \n",
    "        d_ex2 = self.dec_ex2(u_ex2)  \n",
    "        u_ex3 = self.up_ex3(d_ex2)    \n",
    "        d_ex3 = self.dec_ex3(u_ex3)  \n",
    "\n",
    "        out = self.outc(d_ex3)       \n",
    "        return out  # [B,1,256,256]\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建训练/验证 DataLoader\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = RotOnly8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset  (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转增强×2), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. 训练循环（含验证）\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "os.makedirs(\"Train_UP8_rotonly\", exist_ok=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    # ———— 6.1 训练 ————\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)               # [B,1,256,256]\n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inp.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ———— 6.2 验证 ————\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v, tgt_v = inp_v.to(device), tgt_v.to(device)\n",
    "            out_v = model(inp_v)\n",
    "            vloss = criterion(out_v, tgt_v)\n",
    "            val_loss += vloss.item() * inp_v.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 保存模型权重\n",
    "    torch.save(model.state_dict(), f\"Train_UP8_rotonly/UNetUp8_epoch{epoch:02d}.pth\")\n",
    "\n",
    "print(\"训练完毕，模型保存在 Train_UP8_rotonly/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00b2759-2e5a-4a30-918f-b90926b26fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27160\\3706674091.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 16.74 dB, Avg SSIM = 0.4016\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    首先四次 ×2 解码（每次 Vertical×2 + 跳跃连接），\n",
    "    然后三次仅 Vertical×2 解码（无跳跃），\n",
    "    最后 1×1 卷积输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))             # [B, base, 32,256] → [B, base, 16,256]\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))             # [B, base*2,16,256] → [B, base*2,8,256]\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))             # [B, base*4, 8,256] → [B, base*4,4,256]\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))             # [B, base*8, 4,256] → [B, base*8,2,256]\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        # Decoder w/ skip-connections: 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base, (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        # Three extra vertical ×2 upsampling (no skip connections)\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)            # → [B, base, 32, 256]\n",
    "        p1 = self.pool1(e1)          # → [B, base, 16, 256]\n",
    "        e2 = self.enc2(p1)           # → [B, base*2, 16, 256]\n",
    "        p2 = self.pool2(e2)          # → [B, base*2, 8, 256]\n",
    "        e3 = self.enc3(p2)           # → [B, base*4, 8, 256]\n",
    "        p3 = self.pool3(e3)          # → [B, base*4, 4, 256]\n",
    "        e4 = self.enc4(p3)           # → [B, base*8, 4, 256]\n",
    "        p4 = self.pool4(e4)          # → [B, base*8, 2, 256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B, base*16, 2, 256]\n",
    "\n",
    "        # Decoding with skips\n",
    "        u4 = self.up4(b)                                     # → [B, base*8, 4, 256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))           # → [B, base*8, 4, 256]\n",
    "\n",
    "        u3 = self.up3(d4)                                    # → [B, base*4, 8, 256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))           # → [B, base*4, 8, 256]\n",
    "\n",
    "        u2 = self.up2(d3)                                    # → [B, base*2, 16, 256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))           # → [B, base*2, 16, 256]\n",
    "\n",
    "        u1 = self.up1(d2)                                    # → [B, base, 32, 256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))           # → [B, base, 32, 256]\n",
    "\n",
    "        # Three extra vertical upsampling steps\n",
    "        u_ex1  = self.up_ex1(d1)     # → [B, base, 64, 256]\n",
    "        d_ex1  = self.dec_ex1(u_ex1) # → [B, base, 64, 256]\n",
    "\n",
    "        u_ex2  = self.up_ex2(d_ex1)  # → [B, base, 128, 256]\n",
    "        d_ex2  = self.dec_ex2(u_ex2) # → [B, base, 128, 256]\n",
    "\n",
    "        u_ex3  = self.up_ex3(d_ex2)  # → [B, base, 256, 256]\n",
    "        d_ex3  = self.dec_ex3(u_ex3) # → [B, base, 256, 256]\n",
    "\n",
    "        out = self.outc(d_ex3)       # → [B, 1, 256, 256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./Train_UP8_rotonly/UNetUp8_epoch15.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecc60cd-c6cf-4665-ba71-aa77308c088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 12288 (含旋转×2、8 相位下采样×8 = 16×), 验证集: 256 (无增强)\n",
      "Epoch 01 | Train Loss: 0.0025 | Val Loss: 0.0019\n",
      "Epoch 02 | Train Loss: 0.0018 | Val Loss: 0.0019\n",
      "Epoch 03 | Train Loss: 0.0017 | Val Loss: 0.0019\n",
      "Epoch 04 | Train Loss: 0.0017 | Val Loss: 0.0019\n",
      "Epoch 05 | Train Loss: 0.0017 | Val Loss: 0.0019\n",
      "Epoch 06 | Train Loss: 0.0017 | Val Loss: 0.0018\n",
      "Epoch 07 | Train Loss: 0.0017 | Val Loss: 0.0019\n",
      "Epoch 08 | Train Loss: 0.0017 | Val Loss: 0.0019\n",
      "Epoch 09 | Train Loss: 0.0017 | Val Loss: 0.0019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 227\u001b[0m\n\u001b[0;32m    225\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    226\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 227\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inp\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    228\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# ———— 6.2 验证 ————\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转+8相位下采样增强（×16）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotAndPhase8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    对每张原图做 8 种下采样相位以及 0°/90° 两种旋转，总共 16 倍增强。\n",
    "    输入是 1/8 下采样图 (32×256)，相位由 phase (0–7) 控制，\n",
    "    旋转由 rot_flag (0→不旋转, 1→逆时针90°) 控制，\n",
    "    目标是完整旋转后原图 (256×256)。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        # 构建文件名列表\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每张原图返回 16 个样本：8 个 phase × 2 个 rot_flag\n",
    "        return len(self.fns) * 16\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 计算图像索引和增强参数\n",
    "        img_idx = idx // 16\n",
    "        residual = idx % 16\n",
    "        rot_flag = residual // 8      # 0 或 1\n",
    "        phase = residual % 8          # 0–7\n",
    "\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        # 旋转（如果需要）\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        # 下采样：从 phase 开始，每 8 行保留一行 → 32×256\n",
    "        down_arr = arr[phase::8, :]\n",
    "\n",
    "        # 转为 PIL Image\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        # ToTensor: 自动归一化到 [0,1], shape: [C=1,H,W]\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义仅旋转增强的验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    对验证集，不做任何旋转，仅 1/8 下采样与原图配对。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义 UNetUp8 模型（与训练时一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入 [B,1,32,256] → 输出 [B,1,256,256]\n",
    "    七次纵向 ×2 的可训练上采样（含跳跃连接）。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2,1))\n",
    "        self.enc2  = DoubleConv(base, base*2)\n",
    "        self.pool2 = nn.MaxPool2d((2,1))\n",
    "        self.enc3  = DoubleConv(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool2d((2,1))\n",
    "        self.enc4  = DoubleConv(base*4, base*8)\n",
    "        self.pool4 = nn.MaxPool2d((2,1))\n",
    "\n",
    "        # Bottleneck at 2×256\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        # Decoder 2→4→8→16→32 (跳跃连接)\n",
    "        self.up4  = nn.ConvTranspose2d(base*16, base*8, (2,1), (2,1))\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "        self.up3  = nn.ConvTranspose2d(base*8, base*4, (2,1), (2,1))\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2  = nn.ConvTranspose2d(base*4, base*2, (2,1), (2,1))\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1  = nn.ConvTranspose2d(base*2, base,   (2,1), (2,1))\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        # 额外三次纵向可训练上采样：32→64→128→256\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # 最后 1×1 卷积\n",
    "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        e1 = self.enc1(x)     \n",
    "        p1 = self.pool1(e1)   \n",
    "        e2 = self.enc2(p1)    \n",
    "        p2 = self.pool2(e2)   \n",
    "        e3 = self.enc3(p2)    \n",
    "        p3 = self.pool3(e3)   \n",
    "        e4 = self.enc4(p3)    \n",
    "        p4 = self.pool4(e4)   \n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)  \n",
    "\n",
    "        # 解码 (跳跃连接)\n",
    "        u4 = self.up4(b)                            \n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))  \n",
    "        u3 = self.up3(d4)                           \n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))  \n",
    "        u2 = self.up2(d3)                           \n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))  \n",
    "        u1 = self.up1(d2)                           \n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))  \n",
    "\n",
    "        # 额外三次可训练上采样\n",
    "        u_ex1 = self.up_ex1(d1)       \n",
    "        d_ex1 = self.dec_ex1(u_ex1)  \n",
    "        u_ex2 = self.up_ex2(d_ex1)    \n",
    "        d_ex2 = self.dec_ex2(u_ex2)  \n",
    "        u_ex3 = self.up_ex3(d_ex2)    \n",
    "        d_ex3 = self.dec_ex3(u_ex3)  \n",
    "\n",
    "        out = self.outc(d_ex3)       \n",
    "        return out  # [B,1,256,256]\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建新的训练 DataLoader（旋转+8 相位下采样增强）\n",
    "#    验证集仍使用 Plain8xDataset，无增强\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# 用新的 Dataset 扩大训练集 16 倍\n",
    "train_dataset = RotAndPhase8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset           (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转×2、8 相位下采样×8 = 16×), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. 训练循环（含验证）\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "os.makedirs(\"Train_UP8_rotonly\", exist_ok=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    # ———— 6.1 训练 ————\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)               # [B,1,256,256]\n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inp.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ———— 6.2 验证 ————\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v, tgt_v = inp_v.to(device), tgt_v.to(device)\n",
    "            out_v = model(inp_v)\n",
    "            vloss = criterion(out_v, tgt_v)\n",
    "            val_loss += vloss.item() * inp_v.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 保存模型权重\n",
    "    torch.save(model.state_dict(), f\"Train_UP8_rotonly/UNetUp8_epoch{epoch:02d}.pth\")\n",
    "\n",
    "print(\"训练完毕，模型保存在 Train_UP8_rotonly/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb63198-8b9f-4ce4-a2c4-8013969e6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27160\\1512523260.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 15.51 dB, Avg SSIM = 0.2833\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    首先四次 ×2 解码（每次 Vertical×2 + 跳跃连接），\n",
    "    然后三次仅 Vertical×2 解码（无跳跃），\n",
    "    最后 1×1 卷积输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))             # [B, base, 32,256] → [B, base, 16,256]\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))             # [B, base*2,16,256] → [B, base*2,8,256]\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))             # [B, base*4, 8,256] → [B, base*4,4,256]\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))             # [B, base*8, 4,256] → [B, base*8,2,256]\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        # Decoder w/ skip-connections: 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base, (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        # Three extra vertical ×2 upsampling (no skip connections)\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)            # → [B, base, 32, 256]\n",
    "        p1 = self.pool1(e1)          # → [B, base, 16, 256]\n",
    "        e2 = self.enc2(p1)           # → [B, base*2, 16, 256]\n",
    "        p2 = self.pool2(e2)          # → [B, base*2, 8, 256]\n",
    "        e3 = self.enc3(p2)           # → [B, base*4, 8, 256]\n",
    "        p3 = self.pool3(e3)          # → [B, base*4, 4, 256]\n",
    "        e4 = self.enc4(p3)           # → [B, base*8, 4, 256]\n",
    "        p4 = self.pool4(e4)          # → [B, base*8, 2, 256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B, base*16, 2, 256]\n",
    "\n",
    "        # Decoding with skips\n",
    "        u4 = self.up4(b)                                     # → [B, base*8, 4, 256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))           # → [B, base*8, 4, 256]\n",
    "\n",
    "        u3 = self.up3(d4)                                    # → [B, base*4, 8, 256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))           # → [B, base*4, 8, 256]\n",
    "\n",
    "        u2 = self.up2(d3)                                    # → [B, base*2, 16, 256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))           # → [B, base*2, 16, 256]\n",
    "\n",
    "        u1 = self.up1(d2)                                    # → [B, base, 32, 256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))           # → [B, base, 32, 256]\n",
    "\n",
    "        # Three extra vertical upsampling steps\n",
    "        u_ex1  = self.up_ex1(d1)     # → [B, base, 64, 256]\n",
    "        d_ex1  = self.dec_ex1(u_ex1) # → [B, base, 64, 256]\n",
    "\n",
    "        u_ex2  = self.up_ex2(d_ex1)  # → [B, base, 128, 256]\n",
    "        d_ex2  = self.dec_ex2(u_ex2) # → [B, base, 128, 256]\n",
    "\n",
    "        u_ex3  = self.up_ex3(d_ex2)  # → [B, base, 256, 256]\n",
    "        d_ex3  = self.dec_ex3(u_ex3) # → [B, base, 256, 256]\n",
    "\n",
    "        out = self.outc(d_ex3)       # → [B, 1, 256, 256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./Train_UP8_rotonly/UNetUp8_epoch09.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb1c2f1-2e28-4389-855c-a7b30b3a969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 1536 (含旋转增强×2), 验证集: 256 (无增强)\n",
      "Epoch 01 | Train Loss: 0.0182 | Val Loss: 0.0077\n",
      "Epoch 02 | Train Loss: 0.0052 | Val Loss: 0.0044\n",
      "Epoch 03 | Train Loss: 0.0044 | Val Loss: 0.0042\n",
      "Epoch 04 | Train Loss: 0.0041 | Val Loss: 0.0039\n",
      "Epoch 05 | Train Loss: 0.0039 | Val Loss: 0.0039\n",
      "Epoch 06 | Train Loss: 0.0037 | Val Loss: 0.0037\n",
      "Epoch 07 | Train Loss: 0.0037 | Val Loss: 0.0036\n",
      "Epoch 08 | Train Loss: 0.0036 | Val Loss: 0.0036\n",
      "Epoch 09 | Train Loss: 0.0036 | Val Loss: 0.0036\n",
      "Epoch 10 | Train Loss: 0.0036 | Val Loss: 0.0037\n",
      "Epoch 11 | Train Loss: 0.0036 | Val Loss: 0.0036\n",
      "Epoch 12 | Train Loss: 0.0036 | Val Loss: 0.0035\n",
      "Epoch 13 | Train Loss: 0.0035 | Val Loss: 0.0035\n",
      "Epoch 14 | Train Loss: 0.0035 | Val Loss: 0.0035\n",
      "Epoch 15 | Train Loss: 0.0035 | Val Loss: 0.0035\n",
      "训练完毕，模型保存在 Train_UP8_hf_weighted/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转增强（×2）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotOnly8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    仅对原图做 0° 和 90° 旋转两种版本，输入是 1/8 下采样图 (32×256)，\n",
    "    目标是完整旋转后原图 (256×256)。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        # 构建文件名列表\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每张原图返回两个样本：rot=0 或 rot=1\n",
    "        return len(self.fns) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 2\n",
    "        rot_flag = idx % 2  # 0→不旋转, 1→逆时针90°\n",
    "\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        # 旋转（如果需要）\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        # 下采样：竖向每 8 行保留一行 → 32×256\n",
    "        down_arr = arr[::8, :]\n",
    "\n",
    "        # 转为 PIL Image\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        # ToTensor: 自动归一化到 [0,1], shape: [C=1,H,W]\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义仅旋转增强的验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    对验证集，不做任何旋转，仅 1/8 下采样与原图配对。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义 UNetUp8 模型（与训练时一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入 [B,1,32,256] → 输出 [B,1,256,256]\n",
    "    七次纵向 ×2 的可训练上采样（含跳跃连接）。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2,1))\n",
    "        self.enc2  = DoubleConv(base, base*2)\n",
    "        self.pool2 = nn.MaxPool2d((2,1))\n",
    "        self.enc3  = DoubleConv(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool2d((2,1))\n",
    "        self.enc4  = DoubleConv(base*4, base*8)\n",
    "        self.pool4 = nn.MaxPool2d((2,1))\n",
    "\n",
    "        # Bottleneck at 2×256\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        # Decoder 2→4→8→16→32 (跳跃连接)\n",
    "        self.up4  = nn.ConvTranspose2d(base*16, base*8, (2,1), (2,1))\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "        self.up3  = nn.ConvTranspose2d(base*8, base*4, (2,1), (2,1))\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2  = nn.ConvTranspose2d(base*4, base*2, (2,1), (2,1))\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1  = nn.ConvTranspose2d(base*2, base,   (2,1), (2,1))\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        # 额外三次纵向可训练上采样：32→64→128→256\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base,   (2,1), (2,1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # 最后 1×1 卷积\n",
    "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        e1 = self.enc1(x)     \n",
    "        p1 = self.pool1(e1)   \n",
    "        e2 = self.enc2(p1)    \n",
    "        p2 = self.pool2(e2)   \n",
    "        e3 = self.enc3(p2)    \n",
    "        p3 = self.pool3(e3)   \n",
    "        e4 = self.enc4(p3)    \n",
    "        p4 = self.pool4(e4)   \n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)  \n",
    "\n",
    "        # 解码 (跳跃连接)\n",
    "        u4 = self.up4(b)                            \n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))  \n",
    "        u3 = self.up3(d4)                           \n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))  \n",
    "        u2 = self.up2(d3)                           \n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))  \n",
    "        u1 = self.up1(d2)                           \n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))  \n",
    "\n",
    "        # 额外三次可训练上采样\n",
    "        u_ex1 = self.up_ex1(d1)       \n",
    "        d_ex1 = self.dec_ex1(u_ex1)  \n",
    "        u_ex2 = self.up_ex2(d_ex1)    \n",
    "        d_ex2 = self.dec_ex2(u_ex2)  \n",
    "        u_ex3 = self.up_ex3(d_ex2)    \n",
    "        d_ex3 = self.dec_ex3(u_ex3)  \n",
    "\n",
    "        out = self.outc(d_ex3)       \n",
    "        return out  # [B,1,256,256]\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建训练/验证 DataLoader\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = RotOnly8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset  (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转增强×2), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. 训练循环（含高频细节加权的损失）\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "os.makedirs(\"Train_UP8_hf_weighted\", exist_ok=True)\n",
    "\n",
    "# 基础 MSE 损失\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 高频细节加权部分：使用拉普拉斯卷积核提取高频\n",
    "laplacian_kernel = torch.tensor(\n",
    "    [[0.0, -1.0, 0.0],\n",
    "     [-1.0, 4.0, -1.0],\n",
    "     [0.0, -1.0, 0.0]],\n",
    "    device=device\n",
    ").view(1, 1, 3, 3)  # [out_ch=1, in_ch=1, 3, 3]\n",
    "\n",
    "# 高频损失权重（可根据需求调整）\n",
    "lambda_hf = 0.5\n",
    "\n",
    "def high_freq_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    对 pred 和 target 应用拉普拉斯高通滤波，然后计算它们之间的 MSE。\n",
    "    输入 pred, target: [B, 1, H, W]\n",
    "    \"\"\"\n",
    "    # padding=1 保证输出大小不变\n",
    "    pred_lap = F.conv2d(pred, laplacian_kernel, padding=1)\n",
    "    tgt_lap  = F.conv2d(target, laplacian_kernel, padding=1)\n",
    "    return F.mse_loss(pred_lap, tgt_lap)\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    # ———— 6.1 训练 ————\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)  # [B,1,256,256]\n",
    "\n",
    "        # 基础 MSE 损失\n",
    "        mse = criterion(out, tgt)\n",
    "        # 高频细节损失\n",
    "        hf  = high_freq_loss(out, tgt)\n",
    "        # 总损失 = MSE + lambda_hf * 高频损失\n",
    "        loss = mse + lambda_hf * hf\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inp.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ———— 6.2 验证 ————\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v, tgt_v = inp_v.to(device), tgt_v.to(device)\n",
    "            out_v = model(inp_v)\n",
    "\n",
    "            mse_v = criterion(out_v, tgt_v)\n",
    "            hf_v  = high_freq_loss(out_v, tgt_v)\n",
    "            loss_v = mse_v + lambda_hf * hf_v\n",
    "\n",
    "            val_loss += loss_v.item() * inp_v.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 保存模型权重\n",
    "    torch.save(model.state_dict(), f\"Train_UP8_hf_weighted/UNetUp8_epoch{epoch:02d}.pth\")\n",
    "\n",
    "print(\"训练完毕，模型保存在 Train_UP8_hf_weighted/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b271794-f3f0-48e2-b4fd-75b39934c122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_11980\\1023578833.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 16.72 dB, Avg SSIM = 0.4213\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    首先四次 ×2 解码（每次 Vertical×2 + 跳跃连接），\n",
    "    然后三次仅 Vertical×2 解码（无跳跃），\n",
    "    最后 1×1 卷积输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))             # [B, base, 32,256] → [B, base, 16,256]\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))             # [B, base*2,16,256] → [B, base*2,8,256]\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))             # [B, base*4, 8,256] → [B, base*4,4,256]\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))             # [B, base*8, 4,256] → [B, base*8,2,256]\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        # Decoder w/ skip-connections: 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base, (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        # Three extra vertical ×2 upsampling (no skip connections)\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)            # → [B, base, 32, 256]\n",
    "        p1 = self.pool1(e1)          # → [B, base, 16, 256]\n",
    "        e2 = self.enc2(p1)           # → [B, base*2, 16, 256]\n",
    "        p2 = self.pool2(e2)          # → [B, base*2, 8, 256]\n",
    "        e3 = self.enc3(p2)           # → [B, base*4, 8, 256]\n",
    "        p3 = self.pool3(e3)          # → [B, base*4, 4, 256]\n",
    "        e4 = self.enc4(p3)           # → [B, base*8, 4, 256]\n",
    "        p4 = self.pool4(e4)          # → [B, base*8, 2, 256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B, base*16, 2, 256]\n",
    "\n",
    "        # Decoding with skips\n",
    "        u4 = self.up4(b)                                     # → [B, base*8, 4, 256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))           # → [B, base*8, 4, 256]\n",
    "\n",
    "        u3 = self.up3(d4)                                    # → [B, base*4, 8, 256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))           # → [B, base*4, 8, 256]\n",
    "\n",
    "        u2 = self.up2(d3)                                    # → [B, base*2, 16, 256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))           # → [B, base*2, 16, 256]\n",
    "\n",
    "        u1 = self.up1(d2)                                    # → [B, base, 32, 256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))           # → [B, base, 32, 256]\n",
    "\n",
    "        # Three extra vertical upsampling steps\n",
    "        u_ex1  = self.up_ex1(d1)     # → [B, base, 64, 256]\n",
    "        d_ex1  = self.dec_ex1(u_ex1) # → [B, base, 64, 256]\n",
    "\n",
    "        u_ex2  = self.up_ex2(d_ex1)  # → [B, base, 128, 256]\n",
    "        d_ex2  = self.dec_ex2(u_ex2) # → [B, base, 128, 256]\n",
    "\n",
    "        u_ex3  = self.up_ex3(d_ex2)  # → [B, base, 256, 256]\n",
    "        d_ex3  = self.dec_ex3(u_ex3) # → [B, base, 256, 256]\n",
    "\n",
    "        out = self.outc(d_ex3)       # → [B, 1, 256, 256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_hf_weighted_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./Train_UP8_hf_weighted/UNetUp8_epoch15.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20fbc4d7-613d-4102-8c1c-cd4666320289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批量处理完成，结果保存在 test_hf_weighted_post_outputs_YZ\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 强力去除小矩形伪影的后处理 Cell\n",
    "# ----------------------------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 输入/输出文件夹路径（请根据实际路径修改）\n",
    "input_folder  = \"test_hf_weighted_outputs_YZ\"\n",
    "output_folder = \"test_hf_weighted_post_outputs_YZ\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def remove_block_artifacts_only(img_uint8,\n",
    "                                nlm_h=15,\n",
    "                                nlm_template=7,\n",
    "                                nlm_search=21,\n",
    "                                median_ksize=5,\n",
    "                                morph_kernel_size=3):\n",
    "    \"\"\"\n",
    "    仅强力去除图像中的小矩形伪影，流程：\n",
    "      1. 使用 Non-Local Means 去噪，针对小方块噪声\n",
    "      2. 中值滤波去除残余孤立方块\n",
    "      3. 形态学开运算消除小矩形伪影并平滑边缘\n",
    "    返回处理后的 uint8 灰度图\n",
    "    \"\"\"\n",
    "    # 1. Non-Local Means 去除小矩形伪影\n",
    "    #    h 越大去噪越强，template 与 search 可适度增大\n",
    "    denoised = cv2.fastNlMeansDenoising(\n",
    "        img_uint8,\n",
    "        None,\n",
    "        h=nlm_h,\n",
    "        templateWindowSize=nlm_template,\n",
    "        searchWindowSize=nlm_search\n",
    "    )\n",
    "\n",
    "    # 2. 中值滤波：去除剩余的小方块噪声\n",
    "    blurred = cv2.medianBlur(denoised, median_ksize)\n",
    "\n",
    "    # 3. 形态学开运算：用小核去除孤立的矩形伪影，并微平滑\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (morph_kernel_size, morph_kernel_size))\n",
    "    opened = cv2.morphologyEx(blurred, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    return opened\n",
    "\n",
    "# 遍历输入文件夹中的所有图像\n",
    "for fn in sorted(os.listdir(input_folder)):\n",
    "    if not fn.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")):\n",
    "        continue\n",
    "\n",
    "    in_path  = os.path.join(input_folder, fn)\n",
    "    out_path = os.path.join(output_folder, fn)\n",
    "\n",
    "    # 读取灰度图\n",
    "    img = cv2.imread(in_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"无法读取: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    # 调用仅去除小矩形伪影的函数\n",
    "    processed = remove_block_artifacts_only(\n",
    "        img_uint8=img,\n",
    "        nlm_h=15,                 # Non-Local Means 去噪强度\n",
    "        nlm_template=7,           # NLM 模板窗口大小（奇数）\n",
    "        nlm_search=21,            # NLM 搜索窗口大小\n",
    "        median_ksize=5,           # 中值滤波核，去除小块噪声\n",
    "        morph_kernel_size=3       # 形态学开运算核大小\n",
    "    )\n",
    "\n",
    "    # 保存结果\n",
    "    cv2.imwrite(out_path, processed)\n",
    "\n",
    "print(\"批量处理完成，结果保存在\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daccbfbe-3b54-4b02-8699-c0c7ef691a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 1024 张图像的平均 PSNR: 16.1225 dB\n",
      "前 1024 张图像的平均 SSIM: 0.2379\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 强力去除小矩形伪影的后处理 Cell\n",
    "# ----------------------------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 输入/输出文件夹路径（请根据实际路径修改）\n",
    "input_folder  = \"test_hf_weighted_outputs_YZ\"\n",
    "output_folder = \"test_hf_weighted_post_outputs_YZ\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def remove_block_artifacts_only(img_uint8,\n",
    "                                nlm_h=15,\n",
    "                                nlm_template=7,\n",
    "                                nlm_search=21,\n",
    "                                median_ksize=5,\n",
    "                                morph_kernel_size=3):\n",
    "    \"\"\"\n",
    "    仅强力去除图像中的小矩形伪影，流程：\n",
    "      1. 使用 Non-Local Means 去噪，针对小方块噪声\n",
    "      2. 中值滤波去除残余孤立方块\n",
    "      3. 形态学开运算消除小矩形伪影并平滑边缘\n",
    "    返回处理后的 uint8 灰度图\n",
    "    \"\"\"\n",
    "    # 1. Non-Local Means 去除小矩形伪影\n",
    "    #    h 越大去噪越强，template 与 search 可适度增大\n",
    "    denoised = cv2.fastNlMeansDenoising(\n",
    "        img_uint8,\n",
    "        None,\n",
    "        h=nlm_h,\n",
    "        templateWindowSize=nlm_template,\n",
    "        searchWindowSize=nlm_search\n",
    "    )\n",
    "\n",
    "    # 2. 中值滤波：去除剩余的小方块噪声\n",
    "    blurred = cv2.medianBlur(denoised, median_ksize)\n",
    "\n",
    "    # 3. 形态学开运算：用小核去除孤立的矩形伪影，并微平滑\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (morph_kernel_size, morph_kernel_size))\n",
    "    opened = cv2.morphologyEx(blurred, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    return opened\n",
    "\n",
    "# 遍历输入文件夹中的所有图像\n",
    "for fn in sorted(os.listdir(input_folder)):\n",
    "    if not fn.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")):\n",
    "        continue\n",
    "\n",
    "    in_path  = os.path.join(input_folder, fn)\n",
    "    out_path = os.path.join(output_folder, fn)\n",
    "\n",
    "    # 读取灰度图\n",
    "    img = cv2.imread(in_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"无法读取: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    # 调用仅去除小矩形伪影的函数\n",
    "    processed = remove_block_artifacts_only(\n",
    "        img_uint8=img,\n",
    "        nlm_h=15,                 # Non-Local Means 去噪强度\n",
    "        nlm_template=7,           # NLM 模板窗口大小（奇数）\n",
    "        nlm_search=21,            # NLM 搜索窗口大小\n",
    "        median_ksize=5,           # 中值滤波核，去除小块噪声\n",
    "        morph_kernel_size=3       # 形态学开运算核大小\n",
    "    )\n",
    "\n",
    "    # 保存结果\n",
    "    cv2.imwrite(out_path, processed)\n",
    "\n",
    "print(\"批量处理完成，结果保存在\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b888a9f5-c869-4811-ae0d-f92d2917a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 1536 (含旋转增强×2), 验证集: 256 (无增强)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27572\\3156382656.py:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]  Avg Loss: 0.001702\n",
      "  Validation Pixel-MSE: 94.313433   Validation Perceptual Loss: 0.050782\n",
      "\n",
      "Epoch [2/2]  Avg Loss: 0.001680\n",
      "  Validation Pixel-MSE: 94.970843   Validation Perceptual Loss: 0.045691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转增强（×2）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotOnly8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    仅对原图做 0° 和 90° 旋转两种版本，\n",
    "    输入: 1/8 下采样图 (32×256)，目标: 原图 (256×256)。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 2\n",
    "        rot_flag = idx % 2  # 0→不旋转, 1→逆时针90°\n",
    "\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        # 下采样：竖向每 8 行保留一行 → 32×256\n",
    "        down_arr = arr[::8, :]\n",
    "\n",
    "        down_img = Image.fromarray(down_arr)  # 32×256\n",
    "        tgt_img  = Image.fromarray(arr)       # 256×256\n",
    "\n",
    "        inp_t = self.transform(down_img)  # [1,32,256], 自动归一化到 [0,1]\n",
    "        tgt_t = self.transform(tgt_img)   # [1,256,256]\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    验证集不做旋转，只做 1/8 下采样与原图配对。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))\n",
    "        self.enc2  = DoubleConv(base, base*2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        self.enc3  = DoubleConv(base*2, base*4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))\n",
    "        self.enc4  = DoubleConv(base*4, base*8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        # Decoder (含跳跃连接): 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base*16, base*8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base*8, base*4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base*4, base*2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base*2, base,   (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        # 三次只做 Vertical ×2 上采样\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base,   (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base,   (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base,   (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # 最后 1×1 卷积输出\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x);     p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1);    p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2);    p3 = self.pool3(e3)\n",
    "        e4 = self.enc4(p3);    p4 = self.pool4(e4)\n",
    "\n",
    "        b  = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.up4(b);      d4 = self.dec4(torch.cat([u4, e4], dim=1))\n",
    "        u3 = self.up3(d4);     d3 = self.dec3(torch.cat([u3, e3], dim=1))\n",
    "        u2 = self.up2(d3);     d2 = self.dec2(torch.cat([u2, e2], dim=1))\n",
    "        u1 = self.up1(d2);     d1 = self.dec1(torch.cat([u1, e1], dim=1))\n",
    "\n",
    "        u_ex1 = self.up_ex1(d1);  d_ex1 = self.dec_ex1(u_ex1)\n",
    "        u_ex2 = self.up_ex2(d_ex1); d_ex2 = self.dec_ex2(u_ex2)\n",
    "        u_ex3 = self.up_ex3(d_ex2); d_ex3 = self.dec_ex3(u_ex3)\n",
    "\n",
    "        out = self.outc(d_ex3)\n",
    "        return out  # [B, 1, 256, 256]\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建训练/验证 DataLoader\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = RotOnly8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset  (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转增强×2), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. 加载预训练 UNetUp8 作为 Generator\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = UNetUp8().to(device)\n",
    "netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\", map_location=device))\n",
    "netG.train()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7. 构建 VGG19 感知特征提取器（只保留到 conv4_4 ␣features[:36]）\n",
    "# ----------------------------------------\n",
    "vgg_full = models.vgg19(pretrained=True).to(device)\n",
    "vgg_extractor = nn.Sequential(*list(vgg_full.features.children())[:36]).to(device)\n",
    "for param in vgg_extractor.parameters():\n",
    "    param.requires_grad = False  # 冻结 VGG19 的权重\n",
    "\n",
    "# ----------------------------------------\n",
    "# 8. 定义感知损失（Perceptual Loss）与像素损失\n",
    "# ----------------------------------------\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "def perceptual_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pred, target: [B,1,256,256], 假定范围已经在 [0,1]\n",
    "    先复制成 3 通道，做 ImageNet Normalize，然后提取 VGG19 conv4_4 的特征并算 MSE。\n",
    "    \"\"\"\n",
    "    # 1. 复制成 3 通道\n",
    "    pred_rgb = pred.repeat(1, 3, 1, 1)  # [B,3,256,256]\n",
    "    tgt_rgb  = target.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # 2. ImageNet 标准化\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=pred.device).view(1, 3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=pred.device).view(1, 3, 1, 1)\n",
    "    pred_norm = (pred_rgb - mean) / std\n",
    "    tgt_norm  = (tgt_rgb  - mean) / std\n",
    "\n",
    "    # 3. 提取 conv4_4 特征\n",
    "    feat_pred = vgg_extractor(pred_norm)\n",
    "    feat_tgt  = vgg_extractor(tgt_norm)\n",
    "    return F.mse_loss(feat_pred, feat_tgt)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 9. 定义优化器 & 损失权重\n",
    "# ----------------------------------------\n",
    "optimizer = Adam(netG.parameters(), lr=1e-5)  # 训练 lr 可根据情况微调\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# ====== 关键修改：对比原来 lambda_per = 0.01，我们把 lambda_per 调小到 0.005 ====== #\n",
    "lambda_mse = 1.0\n",
    "lambda_per = 0.005  # ← 原来是 0.01，这里先尝试减半，或可微调到 0.002~0.01\n",
    "\n",
    "# 你也可以尝试以下几种备选组合：\n",
    "# 1) lambda_per = 0.003, lambda_mse = 1.0\n",
    "# 2) lambda_per = 0.01, lambda_mse = 1.0 (原始配置)\n",
    "# 3) lambda_per = 0.005, lambda_mse = 1.0  (当前示例)\n",
    "# 4) lambda_per = 0.005, lambda_mse = 0.5  (让感知略占更大比重)\n",
    "# 5) lambda_per = 0.002, lambda_mse = 1.0  (感知更弱)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 10. 训练循环\n",
    "# ----------------------------------------\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    netG.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inp, tgt in train_loader:\n",
    "        inp = inp.to(device)   # [B,1,32,256]\n",
    "        tgt = tgt.to(device)   # [B,1,256,256]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = netG(inp)        # [B,1,256,256]\n",
    "\n",
    "        # a) 像素级 MSE 损失\n",
    "        loss_mse = criterion_mse(out, tgt)\n",
    "\n",
    "        # b) 感知损失\n",
    "        loss_per = perceptual_loss(out, tgt)\n",
    "\n",
    "        # c) 总损失\n",
    "        loss = lambda_mse * loss_mse + lambda_per * loss_per\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inp.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]  Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 11. 验证（这里我们简单计算 Pixel-MSE 与 Perceptual Loss，用于监控）\n",
    "    # ----------------------------------------\n",
    "    netG.eval()\n",
    "    val_mse  = 0.0\n",
    "    val_per  = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v = inp_v.to(device)\n",
    "            tgt_v = tgt_v.to(device)\n",
    "            out_v = netG(inp_v)\n",
    "\n",
    "            val_mse  += F.mse_loss(out_v, tgt_v, reduction=\"sum\").item()\n",
    "            val_per  += perceptual_loss(out_v, tgt_v).item() * inp_v.size(0)\n",
    "\n",
    "    # Pixel-MSE：sum → avg-per-pixel\n",
    "    val_mse /= len(val_loader.dataset)\n",
    "    # Perceptual Loss：先 sum，然后除以张数\n",
    "    val_per /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"  Validation Pixel-MSE: {val_mse:.6f}   Validation Perceptual Loss: {val_per:.6f}\\n\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 12. 保存微调后的 Generator 权重\n",
    "# ----------------------------------------\n",
    "torch.save(netG.state_dict(), \"UNetUp8_finetuned_perceptual.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d548a126-dfbb-46d5-91eb-5186cac07961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27572\\2417918189.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 16.65 dB, Avg SSIM = 0.3803\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    首先四次 ×2 解码（每次 Vertical×2 + 跳跃连接），\n",
    "    然后三次仅 Vertical×2 解码（无跳跃），\n",
    "    最后 1×1 卷积输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))             # [B, base, 32,256] → [B, base, 16,256]\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))             # [B, base*2,16,256] → [B, base*2,8,256]\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))             # [B, base*4, 8,256] → [B, base*4,4,256]\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))             # [B, base*8, 4,256] → [B, base*8,2,256]\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        # Decoder w/ skip-connections: 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base, (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        # Three extra vertical ×2 upsampling (no skip connections)\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)            # → [B, base, 32, 256]\n",
    "        p1 = self.pool1(e1)          # → [B, base, 16, 256]\n",
    "        e2 = self.enc2(p1)           # → [B, base*2, 16, 256]\n",
    "        p2 = self.pool2(e2)          # → [B, base*2, 8, 256]\n",
    "        e3 = self.enc3(p2)           # → [B, base*4, 8, 256]\n",
    "        p3 = self.pool3(e3)          # → [B, base*4, 4, 256]\n",
    "        e4 = self.enc4(p3)           # → [B, base*8, 4, 256]\n",
    "        p4 = self.pool4(e4)          # → [B, base*8, 2, 256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B, base*16, 2, 256]\n",
    "\n",
    "        # Decoding with skips\n",
    "        u4 = self.up4(b)                                     # → [B, base*8, 4, 256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))           # → [B, base*8, 4, 256]\n",
    "\n",
    "        u3 = self.up3(d4)                                    # → [B, base*4, 8, 256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))           # → [B, base*4, 8, 256]\n",
    "\n",
    "        u2 = self.up2(d3)                                    # → [B, base*2, 16, 256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))           # → [B, base*2, 16, 256]\n",
    "\n",
    "        u1 = self.up1(d2)                                    # → [B, base, 32, 256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))           # → [B, base, 32, 256]\n",
    "\n",
    "        # Three extra vertical upsampling steps\n",
    "        u_ex1  = self.up_ex1(d1)     # → [B, base, 64, 256]\n",
    "        d_ex1  = self.dec_ex1(u_ex1) # → [B, base, 64, 256]\n",
    "\n",
    "        u_ex2  = self.up_ex2(d_ex1)  # → [B, base, 128, 256]\n",
    "        d_ex2  = self.dec_ex2(u_ex2) # → [B, base, 128, 256]\n",
    "\n",
    "        u_ex3  = self.up_ex3(d_ex2)  # → [B, base, 256, 256]\n",
    "        d_ex3  = self.dec_ex3(u_ex3) # → [B, base, 256, 256]\n",
    "\n",
    "        out = self.outc(d_ex3)       # → [B, 1, 256, 256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_VGG_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./UNetUp8_finetuned_perceptual.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab0974f-07a1-48e5-bd56-0f4159ec1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 1536 (含旋转增强×2), 验证集: 256 (无增强)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_11980\\2858538556.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\"))  # 改为你的权重路径\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]  Avg Perceptual Loss: 0.050178\n",
      "  Validation Pixel-MSE: 0.002638   Val Perceptual Loss: 0.004688\n",
      "\n",
      "Epoch [2/10]  Avg Perceptual Loss: 0.036201\n",
      "  Validation Pixel-MSE: 0.002249   Val Perceptual Loss: 0.004198\n",
      "\n",
      "Epoch [3/10]  Avg Perceptual Loss: 0.034371\n",
      "  Validation Pixel-MSE: 0.002207   Val Perceptual Loss: 0.004070\n",
      "\n",
      "Epoch [4/10]  Avg Perceptual Loss: 0.033454\n",
      "  Validation Pixel-MSE: 0.002246   Val Perceptual Loss: 0.003961\n",
      "\n",
      "Epoch [5/10]  Avg Perceptual Loss: 0.032817\n",
      "  Validation Pixel-MSE: 0.002278   Val Perceptual Loss: 0.003914\n",
      "\n",
      "Epoch [6/10]  Avg Perceptual Loss: 0.032283\n",
      "  Validation Pixel-MSE: 0.002281   Val Perceptual Loss: 0.003919\n",
      "\n",
      "Epoch [7/10]  Avg Perceptual Loss: 0.031894\n",
      "  Validation Pixel-MSE: 0.002307   Val Perceptual Loss: 0.003826\n",
      "\n",
      "Epoch [8/10]  Avg Perceptual Loss: 0.031609\n",
      "  Validation Pixel-MSE: 0.002292   Val Perceptual Loss: 0.003780\n",
      "\n",
      "Epoch [9/10]  Avg Perceptual Loss: 0.031334\n",
      "  Validation Pixel-MSE: 0.002317   Val Perceptual Loss: 0.003777\n",
      "\n",
      "Epoch [10/10]  Avg Perceptual Loss: 0.031057\n",
      "  Validation Pixel-MSE: 0.002325   Val Perceptual Loss: 0.003752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转增强（×2）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotOnly8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    仅对原图做 0° 和 90° 旋转两种版本，输入是 1/8 下采样图 (32×256)，\n",
    "    目标是完整旋转后原图 (256×256)。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        # 构建文件名列表\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每张原图返回两个样本：rot=0 或 rot=1\n",
    "        return len(self.fns) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 2\n",
    "        rot_flag = idx % 2  # 0→不旋转, 1→逆时针90°\n",
    "\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "\n",
    "        # 旋转（如果需要）\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        # 下采样：竖向每 8 行保留一行 → 32×256\n",
    "        down_arr = arr[::8, :]\n",
    "\n",
    "        # 转为 PIL Image\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        # ToTensor: 自动归一化到 [0,1], shape: [C=1,H,W]\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义仅旋转增强的验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    \"\"\"\n",
    "    对验证集，不做任何旋转，仅 1/8 下采样与原图配对。\n",
    "    \"\"\"\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 定义 UNetUp8 架构（与训练时完全一致）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256]\n",
    "    首先四次 ×2 解码（每次 Vertical×2 + 跳跃连接），\n",
    "    然后三次仅 Vertical×2 解码（无跳跃），\n",
    "    最后 1×1 卷积输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 32→16→8→4→2\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))             # [B, base, 32,256] → [B, base, 16,256]\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))             # [B, base*2,16,256] → [B, base*2,8,256]\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))             # [B, base*4, 8,256] → [B, base*4,4,256]\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))             # [B, base*8, 4,256] → [B, base*8,2,256]\n",
    "\n",
    "        # Bottleneck at [B, base*8, 2,256]\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        # Decoder w/ skip-connections: 2→4→8→16→32\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base, (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        # Three extra vertical ×2 upsampling (no skip connections)\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)            # → [B, base, 32, 256]\n",
    "        p1 = self.pool1(e1)          # → [B, base, 16, 256]\n",
    "        e2 = self.enc2(p1)           # → [B, base*2, 16, 256]\n",
    "        p2 = self.pool2(e2)          # → [B, base*2, 8, 256]\n",
    "        e3 = self.enc3(p2)           # → [B, base*4, 8, 256]\n",
    "        p3 = self.pool3(e3)          # → [B, base*4, 4, 256]\n",
    "        e4 = self.enc4(p3)           # → [B, base*8, 4, 256]\n",
    "        p4 = self.pool4(e4)          # → [B, base*8, 2, 256]\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)      # → [B, base*16, 2, 256]\n",
    "\n",
    "        # Decoding with skips\n",
    "        u4 = self.up4(b)                                     # → [B, base*8, 4, 256]\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))           # → [B, base*8, 4, 256]\n",
    "\n",
    "        u3 = self.up3(d4)                                    # → [B, base*4, 8, 256]\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))           # → [B, base*4, 8, 256]\n",
    "\n",
    "        u2 = self.up2(d3)                                    # → [B, base*2, 16, 256]\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))           # → [B, base*2, 16, 256]\n",
    "\n",
    "        u1 = self.up1(d2)                                    # → [B, base, 32, 256]\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))           # → [B, base, 32, 256]\n",
    "\n",
    "        # Three extra vertical upsampling steps\n",
    "        u_ex1  = self.up_ex1(d1)     # → [B, base, 64, 256]\n",
    "        d_ex1  = self.dec_ex1(u_ex1) # → [B, base, 64, 256]\n",
    "\n",
    "        u_ex2  = self.up_ex2(d_ex1)  # → [B, base, 128, 256]\n",
    "        d_ex2  = self.dec_ex2(u_ex2) # → [B, base, 128, 256]\n",
    "\n",
    "        u_ex3  = self.up_ex3(d_ex2)  # → [B, base, 256, 256]\n",
    "        d_ex3  = self.dec_ex3(u_ex3) # → [B, base, 256, 256]\n",
    "\n",
    "        out = self.outc(d_ex3)       # → [B, 1, 256, 256]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建训练/验证 DataLoader\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = RotOnly8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset  (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转增强×2), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# 假设以下变量已在前面的 Cell 中定义：\n",
    "#   - train_loader, val_loader: 已创建的 DataLoader（输入 inp: [B,1,32,256]，目标 tgt: [B,1,256,256]）\n",
    "#   - UNetUp8: 你的生成器模型定义\n",
    "#   - device: torch.device(\"cuda\" if available else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = UNetUp8().to(device)\n",
    "netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\"))  # 改为你的权重路径\n",
    "netG.train()\n",
    "\n",
    "# 2. 构建 VGG19 感知特征提取器（只保留到 conv4_4，对应 PyTorch features[:36]）\n",
    "vgg_full = models.vgg19(pretrained=True).to(device)\n",
    "vgg_extractor = nn.Sequential(*list(vgg_full.features.children())[:36]).to(device)\n",
    "for param in vgg_extractor.parameters():\n",
    "    param.requires_grad = False  # 冻结 VGG19\n",
    "\n",
    "# 3. 定义纯感知损失\n",
    "def perceptual_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pred, target: [B,1,256,256]，范围假设为 [0,1]\n",
    "    先扩为 3 通道，再做 ImageNet Normalize，最后计算 VGG conv4_4 的 MSE。\n",
    "    \"\"\"\n",
    "    # 扩成 3 通道\n",
    "    pred_rgb = pred.repeat(1, 3, 1, 1)\n",
    "    tgt_rgb  = target.repeat(1, 3, 1, 1)\n",
    "    # ImageNet 标准化\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=pred.device).view(1, 3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=pred.device).view(1, 3, 1, 1)\n",
    "    pred_norm = (pred_rgb - mean) / std\n",
    "    tgt_norm  = (tgt_rgb  - mean) / std\n",
    "    # 提取 conv4_4 特征\n",
    "    feat_pred = vgg_extractor(pred_norm)\n",
    "    feat_tgt  = vgg_extractor(tgt_norm)\n",
    "    return F.mse_loss(feat_pred, feat_tgt)\n",
    "\n",
    "# 4. 定义优化器（学习率调小一些以适应微调）\n",
    "optimizer = Adam(netG.parameters(), lr=1e-5)\n",
    "\n",
    "# 5. 训练参数：只用感知损失，不加像素 MSE\n",
    "num_epochs = 10\n",
    "lambda_per = 1.0  # 纯感知时直接设为 1\n",
    "\n",
    "# 6. 训练循环\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    netG.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inp, tgt in train_loader:\n",
    "        inp = inp.to(device)  # [B,1,32,256]\n",
    "        tgt = tgt.to(device)  # [B,1,256,256]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = netG(inp)       # [B,1,256,256]\n",
    "\n",
    "        # 仅计算感知损失\n",
    "        loss_per = perceptual_loss(out, tgt)\n",
    "        loss = lambda_per * loss_per\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inp.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]  Avg Perceptual Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    # 验证：计算在验证集上的“每像素 MSE”和“感知损失”以监控\n",
    "    netG.eval()\n",
    "    val_mse_pixel = 0.0\n",
    "    val_per_loss  = 0.0\n",
    "    num_pixels = 256 * 256\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v = inp_v.to(device)\n",
    "            tgt_v = tgt_v.to(device)\n",
    "            out_v = netG(inp_v)\n",
    "\n",
    "            # 1) 每像素 MSE\n",
    "            mse_sum = F.mse_loss(out_v, tgt_v, reduction=\"sum\").item()\n",
    "            val_mse_pixel += mse_sum / num_pixels\n",
    "\n",
    "            # 2) 感知损失\n",
    "            val_per_loss += perceptual_loss(out_v, tgt_v).item()\n",
    "\n",
    "    val_mse_pixel /= len(val_loader.dataset)\n",
    "    val_per_loss  /= len(val_loader.dataset)\n",
    "    print(f\"  Validation Pixel-MSE: {val_mse_pixel:.6f}   Val Perceptual Loss: {val_per_loss:.6f}\\n\")\n",
    "\n",
    "# 7. 保存微调后的 Generator\n",
    "torch.save(netG.state_dict(), \"UNetUp8_finetuned_pure_perceptual.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8196205e-04c1-40a4-8070-08a6a1a9cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_11980\\463274245.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 16.35 dB, Avg SSIM = 0.3670\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_Pure_VGG_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./UNetUp8_finetuned_pure_perceptual.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e7dbdb-9877-4a4c-bcfa-c8a389fcde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 1024, 训练集: 1536 (含旋转增强×2), 验证集: 256 (无增强)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27572\\1267510772.py:229: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]  Avg 1-SSIM Loss: 0.349537\n",
      "  Validation Pixel-MSE: 0.006923   Val SSIM: 0.117042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 基于 1-SSIM 作为损失函数的微调 Cell（带 Sigmoid 归一化）\n",
    "# ----------------------------------------\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 准备图像列表并划分 75% 训练 / 25% 验证\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"train_patches_XY\"\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".png\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义只用旋转增强（×2）的 Dataset\n",
    "# ----------------------------------------\n",
    "class RotOnly8xDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // 2\n",
    "        rot_flag = idx % 2\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        if rot_flag == 1:\n",
    "            arr = np.rot90(arr, k=1)\n",
    "\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)  # [1,32,256]\n",
    "        tgt_t = self.transform(tgt_img)   # [1,256,256]\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 定义仅旋转增强的验证集 Dataset（无旋转、不做增强）\n",
    "# ----------------------------------------\n",
    "class Plain8xDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.indices = indices\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        down_arr = arr[::8, :]\n",
    "        down_img = Image.fromarray(down_arr)\n",
    "        tgt_img  = Image.fromarray(arr)\n",
    "\n",
    "        inp_t = self.transform(down_img)\n",
    "        tgt_t = self.transform(tgt_img)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 定义 UNetUp8 架构（加 Sigmoid 限制输出在 [0,1]）\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetUp8(nn.Module):\n",
    "    \"\"\"\n",
    "    输入: [B, 1, 32, 256]  → 输出: [B, 1, 256, 256], 最后加 Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=64):\n",
    "        super().__init__()\n",
    "        self.enc1  = DoubleConv(in_ch, base)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))\n",
    "        self.enc2  = DoubleConv(base, base * 2)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        self.enc3  = DoubleConv(base * 2, base * 4)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))\n",
    "        self.enc4  = DoubleConv(base * 4, base * 8)\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))\n",
    "        self.bottleneck = DoubleConv(base * 8, base * 16)\n",
    "\n",
    "        self.up4  = nn.ConvTranspose2d(base * 16, base * 8, (2, 1), stride=(2, 1))\n",
    "        self.dec4 = DoubleConv(base * 16, base * 8)\n",
    "        self.up3  = nn.ConvTranspose2d(base * 8, base * 4, (2, 1), stride=(2, 1))\n",
    "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
    "        self.up2  = nn.ConvTranspose2d(base * 4, base * 2, (2, 1), stride=(2, 1))\n",
    "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
    "        self.up1  = nn.ConvTranspose2d(base * 2, base,   (2, 1), stride=(2, 1))\n",
    "        self.dec1 = DoubleConv(base * 2, base)\n",
    "\n",
    "        self.up_ex1  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex1 = DoubleConv(base, base)\n",
    "        self.up_ex2  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex2 = DoubleConv(base, base)\n",
    "        self.up_ex3  = nn.ConvTranspose2d(base, base, (2, 1), stride=(2, 1))\n",
    "        self.dec_ex3 = DoubleConv(base, base)\n",
    "\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()  # 最后加 Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "        b  = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.up4(b)\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))\n",
    "        u3 = self.up3(d4)\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))\n",
    "        u2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))\n",
    "        u1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))\n",
    "\n",
    "        u_ex1 = self.up_ex1(d1)\n",
    "        d_ex1 = self.dec_ex1(u_ex1)\n",
    "        u_ex2 = self.up_ex2(d_ex1)\n",
    "        d_ex2 = self.dec_ex2(u_ex2)\n",
    "        u_ex3 = self.up_ex3(d_ex2)\n",
    "        d_ex3 = self.dec_ex3(u_ex3)\n",
    "\n",
    "        out = self.outc(d_ex3)\n",
    "        out = self.sigmoid(out)  # 强制 [0,1]\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. 创建训练/验证 DataLoader\n",
    "# ----------------------------------------\n",
    "batch_size = 8\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = RotOnly8xDataset(patches_folder, train_idxs, transform)\n",
    "val_dataset   = Plain8xDataset  (patches_folder, val_idxs,   transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"总样本数: {len(all_fns)}, 训练集: {len(train_dataset)} (含旋转增强×2), 验证集: {len(val_dataset)} (无增强)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. 定义 SSIM Loss 函数（确保输入在 [0,1]）\n",
    "# ----------------------------------------\n",
    "def gaussian_window(window_size: int, sigma: float, channel: int, device):\n",
    "    _1D_tensor = torch.tensor(\n",
    "        [np.exp(-((x - window_size//2)**2)/(2*sigma**2)) for x in range(window_size)],\n",
    "        dtype=torch.float32, device=device\n",
    "    ).unsqueeze(1)\n",
    "    _2D_window = (_1D_tensor @ _1D_tensor.t()).unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim_loss(img1: torch.Tensor, img2: torch.Tensor, window_size=11, size_average=True):\n",
    "    \"\"\"\n",
    "    img1, img2: [B,1,256,256], 假定值已在 [0,1]\n",
    "    返回 1 - average(SSIM_map)\n",
    "    \"\"\"\n",
    "    _, channel, _, _ = img1.size()\n",
    "    device = img1.device\n",
    "    sigma = 1.5\n",
    "    window = gaussian_window(window_size, sigma, channel, device)\n",
    "\n",
    "    # 均值\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "    mu1_sq  = mu1.pow(2)\n",
    "    mu2_sq  = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    # 方差与协方差\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "    sigma12   = F.conv2d(img1 * img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "\n",
    "    # 常数 (假设像素范围 [0,1])\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    # 逐元素 SSIM map\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "               ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return 1 - ssim_map.mean()\n",
    "    else:\n",
    "        return 1 - ssim_map.view(ssim_map.size(0), -1).mean(1)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7. 微调步骤：只用 1-SSIM 作为损失\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = UNetUp8().to(device)\n",
    "netG.load_state_dict(torch.load(\"Train_UP8_rotonly/UNetUp8_epoch15.pth\", map_location=device))\n",
    "netG.train()\n",
    "\n",
    "optimizer = Adam(netG.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "lambda_ssim = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    netG.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inp, tgt in train_loader:\n",
    "        inp = inp.to(device)  # [B,1,32,256]\n",
    "        tgt = tgt.to(device)  # [B,1,256,256]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = netG(inp)       # [B,1,256,256], 已经在 [0,1] 之间\n",
    "\n",
    "        # 计算 1 - SSIM 损失\n",
    "        loss_ssim = ssim_loss(out, tgt, window_size=11, size_average=True)\n",
    "        loss = lambda_ssim * loss_ssim\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inp.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]  Avg 1-SSIM Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    # 验证：计算在验证集上的“每像素 MSE”和“平均 SSIM”\n",
    "    netG.eval()\n",
    "    val_mse_pixel = 0.0\n",
    "    val_ssim      = 0.0\n",
    "    num_pixels = 256 * 256\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v = inp_v.to(device)\n",
    "            tgt_v = tgt_v.to(device)\n",
    "            out_v = netG(inp_v)\n",
    "\n",
    "            # 每像素 MSE\n",
    "            mse_sum = F.mse_loss(out_v, tgt_v, reduction=\"sum\").item()\n",
    "            val_mse_pixel += mse_sum / num_pixels\n",
    "\n",
    "            # SSIM 值\n",
    "            ssim_val = 1 - ssim_loss(out_v, tgt_v, window_size=11, size_average=True)\n",
    "            val_ssim += ssim_val.item()\n",
    "\n",
    "    val_mse_pixel /= len(val_loader.dataset)\n",
    "    val_ssim      /= len(val_loader.dataset)\n",
    "    print(f\"  Validation Pixel-MSE: {val_mse_pixel:.6f}   Val SSIM: {val_ssim:.6f}\\n\")\n",
    "\n",
    "# 8. 保存微调后的 Generator\n",
    "torch.save(netG.state_dict(), \"UNetUp8_ssim_losstuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94d91f7-4ec7-4eeb-bcc8-bb68e7c2fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\AppData\\Local\\Temp\\ipykernel_27572\\1535367887.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = 13.52 dB, Avg SSIM = 0.1508\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "# ----------------------------------------\n",
    "# 2. 测试集路径 & 输出文件夹\n",
    "# ----------------------------------------\n",
    "test_folder  = \"test_patches_YZ\"       # 包含完整 256×256 切片\n",
    "down_folder  = \"test_down8_YZ\"         # 用于保存 1/8 下采样的 32×256 图\n",
    "output_folder = \"test_SSIM_loss_outputs_YZ\"      # 用于保存 上采样回 256×256\n",
    "os.makedirs(down_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 载入模型权重\n",
    "# ----------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetUp8(in_ch=1, out_ch=1, base=64).to(device)\n",
    "ckpt_path = \"./UNetUp8_ssim_losstuned.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 逐张读取 test_patches_YZ，做 1/8 下采样 → 上采样 → 计算 PSNR/SSIM\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for fn in sorted(os.listdir(test_folder)):\n",
    "    if not fn.lower().endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # 4.1 读取原始 256×256 切片\n",
    "    img_path = os.path.join(test_folder, fn)\n",
    "    orig_img = Image.open(img_path).convert(\"L\")\n",
    "    orig_arr = np.array(orig_img)  # shape: (256, 256)\n",
    "\n",
    "    # 4.2 下采样：竖向每 8 行保留一行 → 得到 32×256\n",
    "    down_arr = orig_arr[::8, :]\n",
    "    down_img = Image.fromarray(down_arr)\n",
    "    down_img.save(os.path.join(down_folder, fn))  # 可选：把下采样图存盘以便查看\n",
    "\n",
    "    # 4.3 转 Tensor 送入网络：[1,1,32,256], 归一化 [0,1]\n",
    "    inp_t = transform(down_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 4.4 上采样推理\n",
    "    with torch.no_grad():\n",
    "        out_t = model(inp_t)  # [1,1,256,256], 值域 [0,1]\n",
    "    out_np = (out_t.squeeze().cpu().numpy() * 255.0).round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4.5 保存输出 256×256 上采样图\n",
    "    Image.fromarray(out_np).save(os.path.join(output_folder, fn))\n",
    "\n",
    "    # 4.6 计算指标\n",
    "    psnr_val = peak_signal_noise_ratio(orig_arr, out_np, data_range=255)\n",
    "    ssim_val = structural_similarity(orig_arr, out_np, data_range=255)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n",
    "# 平均指标\n",
    "if count > 0:\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"\\n[Test YZ] 1/8 → UNetUp8 Epoch15:  Avg PSNR = {avg_psnr:.2f} dB, Avg SSIM = {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"测试文件夹中没有找到 PNG 图像。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac1438-4f2a-47f6-be38-01143aa4f5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
