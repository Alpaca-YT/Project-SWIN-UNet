{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ba4a6-2ac1-4046-bd5c-fda5f21193f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpaca_YT\\anaconda3\\envs\\mphy0041-cw2-pt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始在 cuda 上训练 EDSR (使用高频加权损失)...\n",
      "Epoch 01/15 | Train Loss: 0.020976 | Val Loss: 0.014340\n",
      "  --> Saving best model at epoch 1 with Val Loss: 0.014340\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import timm # 尽管EDSR不直接用timm，但保留以防其他用途\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. 设置和数据加载部分 (保持不变)\n",
    "# ----------------------------------------\n",
    "patches_folder = r\"C:\\Users\\Alpaca_YT\\pythonSet\\lung_slices_dataset\\lung_slice_xy\"\n",
    "output_model_dir = \"Train_EDSR_Full_newlung\" # 更换输出文件夹名称\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "num_epochs = 15\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "all_fns = sorted([f for f in os.listdir(patches_folder) if f.lower().endswith(\".jpg\")])\n",
    "all_indices = list(range(len(all_fns)))\n",
    "train_idxs, val_idxs = train_test_split(all_indices, test_size=0.25, random_state=42)\n",
    "\n",
    "class RotLowHighDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, all_fns_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns_list[i] for i in indices]\n",
    "    def __len__(self): return len(self.fns) * 2\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, rot_flag = idx // 2, idx % 2\n",
    "        fn = self.fns[img_idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        arr = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        if rot_flag == 1: arr = np.rot90(arr, k=1)\n",
    "        down_arr = cv2.resize(arr, (256, 32), interpolation=cv2.INTER_AREA)\n",
    "        up_img = cv2.resize(down_arr, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        inp_t = self.transform(Image.fromarray(up_img))\n",
    "        tgt_t = self.transform(Image.fromarray(arr))\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "class PlainLowHighDataset(Dataset):\n",
    "    def __init__(self, patches_folder, indices, all_fns_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.patches_folder = patches_folder\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.fns = [all_fns_list[i] for i in indices]\n",
    "    def __len__(self): return len(self.fns)\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        img_path = os.path.join(self.patches_folder, fn)\n",
    "        arr = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        down_arr = cv2.resize(arr, (256, 32), interpolation=cv2.INTER_AREA)\n",
    "        up_img = cv2.resize(down_arr, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        inp_t = self.transform(Image.fromarray(up_img))\n",
    "        tgt_t = self.transform(Image.fromarray(arr))\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. 定义 EDSR 模型 (完全替换 SwinUnet 部分)\n",
    "# ----------------------------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"EDSR的残差块，不包含BN层\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.relu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        return x + residual # 残差连接\n",
    "\n",
    "class Upsampler(nn.Module):\n",
    "    \"\"\"EDSR的上采样模块，使用PixelShuffle\"\"\"\n",
    "    def __init__(self, in_channels, scale_factor):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, scale_factor=8, num_res_blocks=16, feature_channels=64):\n",
    "        super(EDSR, self).__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor # 放大倍数，这里是8 (32->256)\n",
    "\n",
    "        # 初始特征提取\n",
    "        self.head = nn.Conv2d(in_channels, feature_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 残差块主体\n",
    "        body = [ResidualBlock(feature_channels) for _ in range(num_res_blocks)]\n",
    "        self.body = nn.Sequential(*body)\n",
    "        \n",
    "        # 最后的卷积层 (在残差块之后，上采样之前)\n",
    "        self.conv_after_res = nn.Conv2d(feature_channels, feature_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # 上采样部分 (实现 8x 放大)\n",
    "        # 256x32 输入，目标 256x256。实际是从32高放大到256高，即 8x\n",
    "        # 我们可以通过多次2x PixelShuffle实现\n",
    "        \n",
    "        # 假设输入是 256x32 (低分辨率输入)\n",
    "        # EDSR的典型实现是在特征空间进行上采样\n",
    "        # 我们的输入是已经插值到 256x256 的，但实际上是从 256x32 来的\n",
    "        # 因此，网络是学习从“插值后的LR”到“HR”的映射\n",
    "\n",
    "        # 这里我们需要思考 scale_factor=8 的应用场景\n",
    "        # 您的数据加载是：\n",
    "        #   1. 原始 HR (256x256) -> 下采样到 LR (256x32)\n",
    "        #   2. LR (256x32) -> 上采样到 LR_bicubic (256x256) 作为网络输入\n",
    "        # 所以，EDSR的任务是从 LR_bicubic (256x256) 学习到 HR (256x256)\n",
    "        # 在这种情况下，我们不需要在网络内部进行显式的 scale_factor=8 的上采样\n",
    "        # EDSR的主体是在一个固定的分辨率上运行，然后最终输出。\n",
    "        # 这里我们将EDSR设计为接收 256x256 伪LR输入，输出 256x256 HR。\n",
    "\n",
    "        # 原始EDSR通常会将 LR 输入直接放大到 HR\n",
    "        # 鉴于您的数据集处理方式，EDSR的“上采样”部分可以简化\n",
    "        # 或者，我们可以让EDSR接收 256x32，然后内部进行 8x 上采样\n",
    "        # 鉴于您提供的数据预处理，我们让EDSR直接处理 256x256 -> 256x256\n",
    "        # 这意味着模型学的是一个图像到图像的映射，而不是低分辨率到高分辨率的传统超分\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 重新考虑EDSR的上采样部分：\n",
    "        # 如果模型直接接收 256x256 的插值后的输入，并输出 256x256 的HR\n",
    "        # 那么EDSR的“上采样”模块就可能不是必需的，或者只需要一个最终的重建卷积。\n",
    "        # 原始EDSR是在低分辨率特征图上进行上采样，然后输出高分辨率图像。\n",
    "        # 您的输入 `inp_t` 已经是 `256x256`。\n",
    "        # 故，EDSR的结构应该是：\n",
    "        # 特征提取 -> 多个残差块 -> 最后的卷积层 -> 输出。\n",
    "\n",
    "        # 这里的 EDSR 不进行内部的上采样操作，因为它接收的输入已经是目标尺寸。\n",
    "        # 它是一个图像到图像的映射网络，目标是消除插值引入的伪影，并恢复细节。\n",
    "        # 这更像是一个图像去噪或增强任务。\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # 最终重建层\n",
    "        self.tail = nn.Conv2d(feature_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # EDSR通常会将输入减去平均值，这里我们假设输入已在0-1范围\n",
    "        # 或者在实际训练时进行数据归一化\n",
    "        \n",
    "        # 初始特征提取\n",
    "        x = self.head(x)\n",
    "        \n",
    "        # 残差块主体\n",
    "        res = self.body(x)\n",
    "        res = self.conv_after_res(res) # 残差块后的卷积\n",
    "        x = x + res # 全局残差连接 (跳过主体部分)\n",
    "\n",
    "        # 最终重建\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. 准备训练\n",
    "# ----------------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = RotLowHighDataset(patches_folder, train_idxs, all_fns, transform)\n",
    "val_dataset = PlainLowHighDataset(patches_folder, val_idxs, all_fns, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 实例化 EDSR 模型 ---\n",
    "# 注意：EDSR在这里被配置为接受 256x256 的输入并输出 256x256 的图像\n",
    "# 因为您的数据加载器已经将 256x32 的 LR 图像双线性插值到了 256x256\n",
    "# 因此，EDSR的任务是“优化”这个插值后的 256x256 图像到真正的 HR 256x256\n",
    "model = EDSR(in_channels=1, out_channels=1, num_res_blocks=32, feature_channels=64).to(device) # 增加残差块数量以提升能力\n",
    "# EDSR通常不使用预训练，所以这里没有pretrained参数\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss() # 基础的MSE损失\n",
    "\n",
    "# --- 定义高频损失所需组件 ---\n",
    "laplacian_kernel = torch.tensor(\n",
    "    [[0.0, -1.0, 0.0],\n",
    "     [-1.0, 4.0, -1.0],\n",
    "     [0.0, -1.0, 0.0]],\n",
    "    device=device, dtype=torch.float32\n",
    ").view(1, 1, 3, 3)\n",
    "\n",
    "lambda_hf = 0.5 # 高频损失的权重\n",
    "\n",
    "def high_freq_loss(pred, target):\n",
    "    \"\"\"计算预测和目标之间高频分量的MSE损失\"\"\"\n",
    "    pred_lap = F.conv2d(pred, laplacian_kernel, padding=1)\n",
    "    tgt_lap  = F.conv2d(target, laplacian_kernel, padding=1)\n",
    "    return F.mse_loss(pred_lap, tgt_lap)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. 训练循环\n",
    "# ----------------------------------------\n",
    "print(f\"开始在 {device} 上训练 EDSR (使用高频加权损失)...\")\n",
    "best_val_loss = float('inf') # 用于保存最佳模型\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inp, tgt in train_loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)\n",
    "        \n",
    "        mse_train = criterion(out, tgt)\n",
    "        hf_train = high_freq_loss(out, tgt)\n",
    "        loss = mse_train + lambda_hf * hf_train # 总损失 = MSE + λ * 高频损失\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inp.size(0)\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp_v, tgt_v in val_loader:\n",
    "            inp_v, tgt_v = inp_v.to(device), tgt_v.to(device)\n",
    "            out_v = model(inp_v)\n",
    "            \n",
    "            mse_val = criterion(out_v, tgt_v)\n",
    "            hf_val = high_freq_loss(out_v, tgt_v)\n",
    "            loss_v = mse_val + lambda_hf * hf_val\n",
    "\n",
    "            val_loss += loss_v.item() * inp_v.size(0)\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        ckpt_path = os.path.join(output_model_dir, f\"EDSR_Full_best.pth\") # 保存最佳模型\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"  --> Saving best model at epoch {epoch} with Val Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    # 也可以选择每个epoch都保存，但为了简洁和效率，这里只保存最佳\n",
    "    # ckpt_path_epoch = os.path.join(output_model_dir, f\"EDSR_Full_epoch{epoch:02d}.pth\")\n",
    "    # torch.save(model.state_dict(), ckpt_path_epoch)\n",
    "\n",
    "print(f\"\\n训练完毕，最佳模型已保存在 '{output_model_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030fe2b-f13a-4243-96e8-26ed696f1e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
